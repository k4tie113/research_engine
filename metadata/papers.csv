id,title,abstract,pdf_url
2509.09680v1,FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark,"The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .",http://arxiv.org/pdf/2509.09680v1
2509.09679v1,ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms,"Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",http://arxiv.org/pdf/2509.09679v1
2509.09677v1,The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs,"Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.",http://arxiv.org/pdf/2509.09677v1
2509.09676v1,SpatialVID: A Large-Scale Video Dataset with Spatial Annotations,"Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.",http://arxiv.org/pdf/2509.09676v1
2509.09674v1,SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning,"Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",http://arxiv.org/pdf/2509.09674v1
2509.09675v1,CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models,"Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.",http://arxiv.org/pdf/2509.09675v1
2509.09672v1,Locality in Image Diffusion Models Emerges from Data Statistics,"Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.",http://arxiv.org/pdf/2509.09672v1
2509.09666v1,Can Understanding and Generation Truly Benefit Together -- or Just Coexist?,"In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising ""aha moment"" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.",http://arxiv.org/pdf/2509.09666v1
2509.09660v1,Steering MoE LLMs via Expert (De)Activation,"Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.",http://arxiv.org/pdf/2509.09660v1
2509.09658v1,Measuring Epistemic Humility in Multimodal Large Language Models,"Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a ""None of the above""
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.",http://arxiv.org/pdf/2509.09658v1
2509.09653v1,Towards A High-Performance Quantum Data Center Network Architecture,"Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.",http://arxiv.org/pdf/2509.09653v1
2509.09651v1,Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations,"We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.",http://arxiv.org/pdf/2509.09651v1
2509.09650v1,All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens,"Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.",http://arxiv.org/pdf/2509.09650v1
2509.09647v1,Reconstructing the origin of black hole mergers using sparse astrophysical models,"The astrophysical origin of binary black hole mergers discovered by LIGO and
Virgo remains uncertain. Efforts to reconstruct the processes that lead to
mergers typically rely on either astrophysical models with fixed parameters, or
continuous analytical models that can be fit to observations. Given the
complexity of astrophysical formation mechanisms, these methods typically
cannot fully take into account model uncertainties, nor can they fully capture
the underlying processes. Here, we present a merger population analysis that
can take a discrete set of simulated model distributions as its input to
interpret observations. The analysis can take into account multiple formation
scenarios as fractional contributors to the total set of observations, and can
naturally account for model uncertainties. We apply this technique to
investigate the origin of black hole mergers observed by LIGO Virgo.
Specifically, we consider a model of AGN assisted black hole merger
distributions, exploring a range of AGN parameters along with several {{SEVN}}
population synthesis models that vary in common envelope efficiency parameter
($\alpha$) and metallicity ($Z$). We estimate the posterior distributions for
AGN+SEVN models using $87$ BBH detections from the $O1--O3$ observation runs.
The inferred total merger rate is $46.2 {Gpc}^{-3} {yr}^{-1}$, with the AGN
sub-population contributing $21.2{Gpc}^{-3}{yr}^{-1}$ and the SEVN
sub-population contributing $25.0 {Gpc}^{-3} {yr}^{-1}$.",http://arxiv.org/pdf/2509.09647v1
2509.09645v1,Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character,"When someone sends us a thoughtful message, we naturally form judgments about
their character. But what happens when that message carries a label indicating
it was written with the help of AI? This paper investigates how the appearance
of AI assistance affects our perceptions of message senders. Adding nuance to
previous research, through two studies (N=399) featuring vignette scenarios, we
find that AI-assistance labels don't necessarily make people view senders
negatively. Rather, they dampen the strength of character signals in
communication. We show that when someone sends a warmth-signalling message
(like thanking or apologizing) without AI help, people more strongly categorize
the sender as warm. At the same time, when someone sends a coldness-signalling
message (like bragging or blaming) without assistance, people more confidently
categorize them as cold. Interestingly, AI labels weaken both these
associations: An AI-assisted apology makes the sender appear less warm than if
they had written it themselves, and an AI-assisted blame makes the sender
appear less cold than if they had composed it independently. This supports our
signal diagnosticity explanation: messages labeled as AI-assisted are viewed as
less diagnostic than messages which seem unassisted. We discuss how our
findings shed light on the causal origins of previously reported observations
in AI-Mediated Communication.",http://arxiv.org/pdf/2509.09645v1
2509.09644v1,RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems,"This paper investigates the data collection enhancement problem in a
reconfigurable intelligent surface (RIS)-empowered intelligent consumer
transportation system (ICTS). We propose a novel framework where a data center
(DC) provides energy to pre-configured roadside unit (RSU) pairs during the
downlink stage. While in the uplink stage, these RSU pairs utilize a hybrid
rate-splitting multiple access (RSMA) and time-division multiple access (TDMA)
protocol to transmit the processed data to the DC, while simultaneously
performing local data processing using the harvested energy. Our objective is
to maximize the minimal processed data volume of the RSU pairs by jointly
optimizing the RIS downlink and uplink phase shifts, the transmit power of the
DC and RSUs, the RSU computation resource allocation, and the time slot
allocation. To address the formulated non-convex problem, we develop an
efficient iterative algorithm integrating alternating optimization and
sequential rank-one constraint relaxation methods. Extensive simulations
demonstrate that the proposed algorithm significantly outperforms baseline
schemes under diverse scenarios, validating its effectiveness in enhancing the
data processing performance for intelligent transportation applications.",http://arxiv.org/pdf/2509.09644v1
2509.09642v1,Resource quantification for programming low-depth quantum circuits,"Noisy intermediate-scale quantum (NISQ) devices pave the way to implement
quantum algorithms that exhibit supremacy over their classical counterparts.
Due to the intrinsic noise and decoherence in the physical system, NISQ
computations are naturally modeled as large-size but low-depth quantum
circuits. Practically, to execute such quantum circuits, we need to pass
commands to a programmable quantum computer. Existing programming approaches,
dedicated to generic unitary transformations, are inefficient in terms of the
computational resources under the low-depth assumption and remain far from
satisfactory. As such, to realize NISQ algorithms, it is crucial to find an
efficient way to program low-depth circuits as the qubit number $N$ increases.
Here, we investigate the gate complexity and the size of quantum memory (known
as the program cost) required to program low-depth brickwork circuits. We
unveil a $\sim N \text{poly} \log N$ worst-case program cost of universal
programming of low-depth brickwork circuits in the large $N$ regime, which is a
tight characterization. Moreover, we analyze the trade-off between the cost of
describing the layout of local gates and the cost of programming them to the
targeted unitaries via the light-cone argument. Our findings suggest that
faithful gate-wise programming is optimal in the low-depth regime.",http://arxiv.org/pdf/2509.09642v1
2509.09638v1,CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype,"With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.",http://arxiv.org/pdf/2509.09638v1
2509.09632v1,Nonlinear Independent Component Analysis Scheme and its application to gravitational wave data analysis,"Noise subtraction is a crucial process in gravitational wave (GW) data
analysis to improve the sensitivity of interferometric detectors. While linear
noise coupling has been extensively studied and successfully mitigated using
methods such as Wiener filtering, subtraction of non-linearly coupled and
non-stationary noise remains a significant challenge. In this work, we propose
a novel independent component analysis (ICA)-based framework designed to
address non-linear coupling in noise subtraction. Building upon previous
developments, we derive a method to estimate general quadratic noise coupling
while maintaining computational transparency compared to machine learning
approaches. The proposed method is tested with simulated data and real GW
strain data from KAGRA. Our results demonstrate the potential of this framework
to effectively mitigate complex noise structures, providing a promising avenue
for improving the sensitivity of GW detectors.",http://arxiv.org/pdf/2509.09632v1
2509.09631v1,DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech,"Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.",http://arxiv.org/pdf/2509.09631v1
2509.09630v1,I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection,"Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.",http://arxiv.org/pdf/2509.09630v1
2509.09629v1,Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems,"The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.",http://arxiv.org/pdf/2509.09629v1
2509.09626v1,Environmental vs. intrinsic quenching at cosmic noon: Predictions from cosmological hydrodynamical simulations for VLT-MOONRISE,"We present an investigation into the quenching of simulated galaxies across
cosmic time, honing in on the role played by both intrinsic and environmental
mechanisms at different epochs. In anticipation of VLT-MOONRISE, the first
wide-field spectroscopic galaxy survey to target cosmic noon, this work
provides clear predictions to compare to the future observations. We
investigate the quenching of centrals, high-mass satellites, and low-mass
satellites from two cosmological hydrodynamical simulations: IllustrisTNG and
EAGLE. Satellites are split according to bespoke mass thresholds, designed to
separate environmental and intrinsic quenching mechanisms. To determine the
best parameter for predicting quiescence, we apply a Random Forest
classification analysis for each galaxy class at each epoch. The Random Forest
classification determines supermassive black hole mass as the best predictor of
quiescence in centrals and high-mass satellites. Alternatively, the quenching
of low-mass satellites is best predicted by group halo mass, at all epochs.
Additionally, we investigate the evolution in the dependence of the quenched
fraction with various parameters, revealing a more complex picture. There is
strong evidence for the rejuvenation of star formation from z = 2 to z = 0 in
EAGLE, but not in IllustrisTNG. The starkest discrepancy between simulations
rests in the mass threshold analysis. While IllustrisTNG predicts the existence
of environmentally quenched satellites visible within the survey limits of
MOONRISE, EAGLE does not. Hence, MOONRISE will provide critical data that is
needed to evaluate current models, and constrain future models, of quenching
processes.",http://arxiv.org/pdf/2509.09626v1
2509.09623v1,Extending orders to types,"Given an ordered structure, we study a natural way to extend the order to
preorders on type spaces. For definably complete, linearly ordered structures,
we give a characterisation of the preorder on the space of 1-types. We apply
these results to the divisibility preorder on ultrafilters, giving an
independence result about the suborder consisting of ultrafilters with only one
fixed prime divisor, as well as a classification of ultrafilters with finitely
many prime divisors.",http://arxiv.org/pdf/2509.09623v1
2509.09621v1,Credible Scores,"We study cheap talk with simple language, where the sender communicates using
a score that aggregates a multidimensional state. Both the sender and the
receiver share the same payoffs, given by a quadratic loss function. We show
that the restriction to scores introduces strategic considerations. First,
equilibrium payoffs can be strictly lower than those achievable under
commitment to a scoring rule. Second, we prove that any equilibrium score must
be either linear or discrete. Finally, assuming normally distributed states, we
fully characterize the set of equilibrium linear scores, which includes both
the ex-ante best and the worst linear scores.",http://arxiv.org/pdf/2509.09621v1
2509.09614v1,LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering,"The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.",http://arxiv.org/pdf/2509.09614v1
2509.09606v1,A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks,"Accurate pathloss prediction is essential for the design and optimization of
UAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches
have shown strong potential, their generalization across diverse environments,
robustness to noisy inputs, and sensitivity to UAV altitude remain
underexplored. To address these challenges, we propose a UNet-based deep
learning architecture that combines multi-scale feature extraction,
convolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)
bottleneck for efficient context aggregation. The model predicts pathloss maps
from log-distance, line-of-sight (LOS) mask, and building mask inputs. In
addition, we develop a fully vectorized LOS mask computation algorithm that
significantly accelerates pre-processing and enables large-scale dataset
generation. Extensive evaluations on both in-house ray-tracing data and the
RadioMapSeer benchmark demonstrate that the proposed model outperforms several
state-of-the-art baselines in accuracy and efficiency. All source code is
publicly released to support reproducibility and future research.",http://arxiv.org/pdf/2509.09606v1
2509.09605v1,Multiwavelength observations of a new black-widow millisecond pulsar PSR J1544-2555,"We report the discovery of a new black-widow millisecond pulsar, PSR
J1544-2555, associated with the Fermi-LAT source 4FGL J1544.2-2554. Optical,
radio, and gamma-ray observations confirmed its nature as a compact spider
binary system. Optical photometry from ULTRACAM revealed a \(\sim\)2.7-hour
orbital period, guiding MeerKAT observations that detected \(\sim\)2.4-ms radio
pulsations. Subsequent timing campaigns using the Murriyang Parkes Telescope,
the Effelsberg 100-m Radio Telescope, and the Nan\c{c}ay Radio Telescope
allowed us to obtain a preliminary timing solution, which enabled us to find
gamma-ray pulsations. The final timing solution, spanning 16 years of Fermi-LAT
gamma-ray data, also displays orbital period variations typical of spider
pulsars. X-ray observations from eROSITA indicate non-thermal emission, but the
relatively low count rate prohibits the search for X-ray pulsations. Optical
light curve modelling using Icarus suggests the asymmetry is best explained by
a spot model, where uneven heating creates localised temperature variations on
the companion. While the optical spectra we obtained are compatible with the
physical properties we infer for the companion star, they were not of
sufficient signal-to-noise to allow for radial velocity measurements, thus
limiting constraints on the neutron star's mass. The observed bluer colour near
the light curve minimum suggests possible non-thermal emission from
intra-binary shocks, supported by the presence of an X-ray source. This
discovery exemplifies the proven capability of the Fermi-LAT catalogue in
identifying millisecond pulsar candidates and highlights the role of optical
surveys in detecting variable sources suitable for radio follow-up.",http://arxiv.org/pdf/2509.09605v1
2509.09603v1,Fault-tolerant transformations of spacetime codes,"Recent advances in quantum error-correction (QEC) have shown that it is often
beneficial to understand fault-tolerance as a dynamical process, a circuit with
redundant measurements that help correct errors, rather than as a static code
equipped with a syndrome extraction circuit. Spacetime codes have emerged as a
natural framework to understand error correction at the circuit level while
leveraging the traditional QEC toolbox. Here, we introduce a framework based on
chain complexes and chain maps to model spacetime codes and transformations
between them. We show that stabilizer codes, quantum circuits, and decoding
problems can all be described using chain complexes, and that the equivalence
of two spacetime codes can be characterized by specific maps between chain
complexes, the fault-tolerant maps, that preserve the number of encoded qubits,
fault distance, and minimum-weight decoding problem. As an application of this
framework, we extend the foliated cluster state construction from stabilizer
codes to any spacetime code, showing that any Clifford circuit can be
transformed into a measurement-based protocol with the same fault-tolerant
properties. To this protocol, we associate a chain complex which encodes the
underlying decoding problem, generalizing previous cluster state complex
constructions. Our method enables the construction of cluster states from
non-CSS, subsystem, and Floquet codes, as well as from logical Clifford
operations on a given code.",http://arxiv.org/pdf/2509.09603v1
2509.09602v1,LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination,"Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.",http://arxiv.org/pdf/2509.09602v1
2509.09601v1,Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing,"Extracting information from big data sets, both real and simulated, is a
modern hallmark of the physical sciences. In practice, students face barriers
to learning ``Big Data'' methods in undergraduate physics and astronomy
curricula. As an attempt to alleviate some of these challenges, we present a
simple, farm-to-table data analysis pipeline that can collect, process, and
plot data from the 800k entries common to the arXiv preprint repository and the
bibliographical database inSpireHEP. The pipeline employs contemporary research
practices and can be implemented using open-sourced Python libraries common to
undergraduate courses on Scientific Computing. To support the use such
pipelines in classroom contexts, we make public an example implementation,
authored by two undergraduate physics students, that runs on off-the-shelf
laptops. For advanced students, we discuss applications of the pipeline,
including for online DAQ monitoring and commercialization.",http://arxiv.org/pdf/2509.09601v1
2509.09597v1,Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication,"Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.",http://arxiv.org/pdf/2509.09597v1
2509.09596v1,How much are LLMs changing the language of academic papers after ChatGPT? A multi-database and full text analysis,"This study investigates how Large Language Models (LLMs) are influencing the
language of academic papers by tracking 12 LLM-associated terms across six
major scholarly databases (Scopus, Web of Science, PubMed, PubMed Central
(PMC), Dimensions, and OpenAlex) from 2015 to 2024. Using over 2.4 million PMC
open-access publications (2021-July 2025), we also analysed full texts to
assess changes in the frequency and co-occurrence of these terms before and
after ChatGPT's initial public release. Across databases, delve (+1,500%),
underscore (+1,000%), and intricate (+700%) had the largest increases between
2022 and 2024. Growth in LLM-term usage was much higher in STEM fields than in
social sciences and arts and humanities. In PMC full texts, the proportion of
papers using underscore six or more times increased by over 10,000% from 2022
to 2025, followed by intricate (+5,400%) and meticulous (+2,800%). Nearly half
of all 2024 PMC papers using any LLM term also included underscore, compared
with only 3%-14% of papers before ChatGPT in 2022. Papers using one LLM term
are now much more likely to include other terms. For example, in 2024,
underscore strongly correlated with pivotal (0.449) and delve (0.311), compared
with very weak associations in 2022 (0.032 and 0.018, respectively). These
findings provide the first large-scale evidence based on full-text publications
and multiple databases that some LLM-related terms are now being used much more
frequently and together. The rapid uptake of LLMs to support scholarly
publishing is a welcome development reducing the language barrier to academic
publishing for non-English speakers.",http://arxiv.org/pdf/2509.09596v1
2509.09595v1,Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis,"Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.",http://arxiv.org/pdf/2509.09595v1
2509.09593v1,Fluent but Unfeeling: The Emotional Blind Spots of Language Models,"The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.",http://arxiv.org/pdf/2509.09593v1
2509.09591v1,Numerical modelling of a partially loaded intermodal container freight train passing through a tunnel,"The bluff nature of a freight train locomotive, coupled with large gaps
created between different wagon formations and loaded goods, influence the
overall pressure wave pattern generated as the train passes through a tunnel.
Typically, 1D models are used to predict the patterns and properties of tunnel
pressure wave formations. However, accurate modelling of regions of separation
at the head of the blunted containers and at unloaded gap sections is essential
for precise predictions of pressure magnitudes. This has traditionally been
difficult to capture with 1D models. Furthermore, achieving this accuracy
through 3D computational methods demands exceptional mesh quality, significant
computational resources, and the careful selection of numerical models. This
paper evaluates various numerical models to capture these complexities within
regions of flow separation. Findings have supported the development of a new 1D
programme to calculate the pressure wave generated by a freight locomotive
entering a tunnel, and is here further extended to consider the discontinuities
of the train body created by intermodal container loading patterns, by
implementing new mesh system and boundary conditions into the 1D programme. A
parameterisation study for different loading configurations is also presented
to improve the overall programme adaptability, and the relationship between
predetermined parameters and gap length is investigated. We validate the
effectiveness of the improved 1D model through comprehensive Large Eddy
Simulation (LES) results and conduct an extensive parameterisation study to
enhance its applicability across various loading configurations. Consequently,
this research bridges the gap in freight train tunnel aerodynamics, offering a
versatile 1D numerical tool for accurate pressure wave prediction.",http://arxiv.org/pdf/2509.09591v1
2509.09588v1,Bulk Thermal Conductance of the 5/2 and 7/3 Fractional Quantum Hall States in the Corbino Geometry,"In this work, making use of time-resolved in situ Joule heating of a
two-dimensional electron gas (2DEG) in the Corbino geometry, we report bulk
thermal conductance measurements for the {\nu} = 5/2 and {\nu} = 7/3 fractional
quantum Hall (FQH) states for electron temperatures ranging from 20 to 150 mK.
We compare our findings with a recent study by Melcer et al. [Nature 625, 489
(2024)] that observed a finite bulk thermal conductivity \k{appa}xx in FQH
states. In spite of the large size difference and the vastly different
experimental schemes used to extract \k{appa}xx, we find in large part that
both experiments yield similar results and conclude that the bulk of FQH states
thermally conducts and violate the Wiedemann-Franz law by a wide margin. Slight
discrepancies between both studies are further discussed in terms of
particle-hole symmetry in the vicinity of the 5/2 and 7/3 FQH states.",http://arxiv.org/pdf/2509.09588v1
2509.09584v1,Visual Grounding from Event Cameras,"Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.",http://arxiv.org/pdf/2509.09584v1
2509.09583v1,Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking,"Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.",http://arxiv.org/pdf/2509.09583v1
2509.09582v1,The emergence of globular clusters and globular-cluster-like dwarfs,"Globular clusters (GCs) are among the oldest and densest stellar systems in
the Universe, yet how they form remains a mystery. Here we present a suite of
cosmological simulations in which both dark-matter-free GCs and
dark-matter-rich dwarf galaxies naturally emerge in the Standard Cosmology. We
show that these objects inhabit distinct locations in the size-luminosity plane
and that they have similar ages, age spread, metallicity and metallicity spread
to globulars and dwarfs in the nearby Universe. About half of our simulated
globulars form by means of regular star formation near the centres of their
host dwarf, with the rest forming further out, triggered by mergers. The latter
are more tidally isolated and more likely to survive to the present day.
Finally, our simulations predict the existence of a new class of object that we
call 'globular-cluster-like dwarfs' (GCDs). These form from a single,
self-quenching, star-formation event in low-mass dark-matter halos at high
redshift and have observational properties intermediate between globulars and
dwarfs. We identify several dwarfs in our Galaxy, such as Reticulum II (refs.
2-4), that could be in this new class. If so, they promise unprecedented
constraints on dark-matter models and new sites to search for metal-free stars.",http://arxiv.org/pdf/2509.09582v1
2509.09581v1,Programmable 200 GOPS Hopfield-inspired photonic Ising machine,"Ising machines offer a compelling approach to addressing NP-hard problems,
but physical realizations that are simultaneously scalable, reconfigurable,
fast, and stable remain elusive. Quantum annealers, like D-Wave's cryogenic
hardware, target combinatorial optimization tasks, but quadratic scaling of
qubit requirements with problem size limits their scalability on dense graphs.
Here, we introduce a programmable, stable, room-temperature optoelectronic
oscillator (OEO)-based Ising machine with linear scaling in spin
representation. Inspired by Hopfield networks, our architecture solves
fully-connected problems with up to 256 spins (65,536 couplings), and $>$41,000
spins (205,000+ couplings) if sparse. Our system leverages cascaded thin-film
lithium niobate modulators, a semiconductor optical amplifier, and a digital
signal processing (DSP) engine in a recurrent time-encoded loop, demonstrating
potential $>$200 giga-operations per second for spin coupling and nonlinearity.
This platform achieves the largest spin configuration in an OEO-based photonic
Ising machine, enabled by high intrinsic speed. We experimentally demonstrate
best-in-class solution quality for Max-Cut problems of arbitrary graph
topologies (2,000 and 20,000 spins) among photonic Ising machines and obtain
ground-state solutions for number partitioning and lattice protein folding -
benchmarks previously unaddressed by photonic systems. Our system leverages
inherent noise from high baud rates to escape local minima and accelerate
convergence. Finally, we show that embedding DSP - traditionally used in
optical communications - within optical computation enhances convergence and
solution quality, opening new frontiers in scalable, ultrafast computing for
optimization, neuromorphic processing, and analog AI.",http://arxiv.org/pdf/2509.09581v1
2509.09577v1,Exactly Solvable Model of Random Walks with Stochastic Exchange,"We solve exactly the non-equilibrium dynamics of two discrete random walkers
moving in channels with transition rates $p \neq q$ that swap positions at a
rate $s$. We compute exactly the joint probability distribution $P_{n,m}(t)$
for the walkers, revealing the existence of two dynamical crossovers. The first
signals the passage from independent diffusion to a swap-dominated regime where
the particles act as identical random walkers swapping positions. The second
crossover occurs when both channels become indistinguishable and the walkers
move around the same position. Furthermore, we demonstrate the existence of a
persistent spatial anisotropy defined by the difference between the second
moments of the probability distributions in the two channels. Our results may
provide a quantitative framework to understand diverse systems. In biology, it
is motivated by motor proteins (kinesin/dynein) exchanging cargo leadership,
membrane receptors swapping binding partners, or brain synapses with
activity-dependent plasticity. In finance, it models traders with distinct risk
profiles swapping positions in limit-order books, or volatility spillover
between coupled markets. These diverse systems share a unifying theme: exchange
processes mediate macroscopic correlations despite individual heterogeneity.",http://arxiv.org/pdf/2509.09577v1
2509.09574v1,Human-in-the-loop Learning Through Decentralized Communication Mechanisms,"Information sharing platforms like TripAdvisor and Waze involve human agents
as both information producers and consumers. All these platforms operate in a
centralized way to collect agents' latest observations of new options (e.g.,
restaurants, hotels, travel routes) and share such information with all in real
time. However, after hearing the central platforms' live updates, many human
agents are found selfish and unwilling to further explore unknown options for
the benefit of others in the long run. To regulate the human-in-the-loop
learning (HILL) game against selfish agents' free-riding, this paper proposes a
paradigm shift from centralized to decentralized way of operation that forces
agents' local explorations through restricting information sharing. When game
theory meets distributed learning, we formulate our decentralized communication
mechanism's design as a new multi-agent Markov decision process (MA-MDP), and
derive its analytical condition to outperform today's centralized operation. As
the optimal decentralized communication mechanism in MA-MDP is NP-hard to
solve, we present an asymptotically optimal algorithm with linear complexity to
determine the mechanism's timing of intermittent information sharing. Then we
turn to non-myopic agents who may revert to even over-explore, and adapt our
mechanism design to work. Simulation experiments using real-world dataset
demonstrate the effectiveness of our decentralized mechanisms for various
scenarios.",http://arxiv.org/pdf/2509.09574v1
2509.09561v1,Mechanism Design with Outliers and Predictions,"We initiate the study of mechanism design with outliers, where the designer
can discard $z$ agents from the social cost objective. This setting is
particularly relevant when some agents exhibit extreme or atypical preferences.
As a natural case study, we consider facility location on the line: $n$
strategic agents report their preferred locations, and a mechanism places a
facility to minimize a social cost function. In our setting, the $z$ agents
farthest from the chosen facility are excluded from the social cost. While it
may seem intuitive that discarding outliers improves efficiency, our results
reveal that the opposite can hold.
  We derive tight bounds for deterministic strategyproof mechanisms under the
two most-studied objectives: utilitarian and egalitarian social cost. Our
results offer a comprehensive view of the impact of outliers. We first show
that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded
approximation for either objective. For egalitarian cost, selecting the $(z +
1)$-th order statistic is strategyproof and 2-approximate. In fact, we show
that this is best possible by providing a matching lower bound. Notably, this
lower bound of 2 persists even when the mechanism has access to a prediction of
the optimal location, in stark contrast to the setting without outliers. For
utilitarian cost, we show that strategyproof mechanisms cannot effectively
exploit outliers, leading to the counterintuitive outcome that approximation
guarantees worsen as the number of outliers increases. However, in this case,
access to a prediction allows us to design a strategyproof mechanism achieving
the best possible trade-off between consistency and robustness. Finally, we
also establish lower bounds for randomized mechanisms that are truthful in
expectation.",http://arxiv.org/pdf/2509.09561v1
2509.09560v1,Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution,"Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary ""thinking"" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.",http://arxiv.org/pdf/2509.09560v1
2509.09551v1,Morphologies of caustics studied by catastrophe charged-particle optics,"This paper explores the topologies of caustics observed in instruments that
employ charged particles, such as electron and ion microscopes. These
geometrical figures are studied here using catastrophe theory. The application
of this geometrical theory to our optical situation has enabled us to
analytically reproduce the behaviours of various caustics. The interest lies
mainly in the universal nature of these results since our treatment requires no
prior knowledge of the optical configuration, but only a smart definition of
the control space. This universal approach has finally made it possible to
extract mathematical relationships between the aberration coefficients of any
optical system, which were hidden by the complexity of optical trajectories but
revealed by the set of catastrophes in the control space. These results provide
a glimpse for future applications of caustics in the development of new
corrected optical systems, especially for ions-based devices.",http://arxiv.org/pdf/2509.09551v1
2509.09550v1,Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates,"Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.",http://arxiv.org/pdf/2509.09550v1
2509.09544v1,Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025),"Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.",http://arxiv.org/pdf/2509.09544v1
2509.09541v1,Compositional Concept Generalization with Variational Quantum Circuits,"Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.",http://arxiv.org/pdf/2509.09541v1
2509.09538v1,Entanglement phases and phase transitions in monitored free fermion system due to localizations,"In recent years, the presence of local potentials has significantly enriched
and diversified the entanglement patterns in monitored free fermion systems. In
our approach, we employ the stochastic Schr\""odinger equation to simulate a
one-dimensional spinless fermion system under continuous measurement and local
potentials. By averaging the steady-state entanglement entropy over many
quantum trajectories, we investigate its dependence on measurement and
localization parameters. We used a phenomenological model to interpret the
numerical results, and the results show that the introduction of local
potentials does not destroy the universality class of the entanglement phase
transition, and that the phase boundary is jointly characterized by the
measurement process and the localization mechanism. This work offers a new
perspective on the characterization of the entanglement phase boundary arising
from the combined effects of measurement and localization, and provides
criteria for detecting this novel phase transition in cold atom systems,
trapped ions, and quantum dot arrays.",http://arxiv.org/pdf/2509.09538v1
2509.09535v1,Unified Framework for Hybrid Aleatory and Epistemic Uncertainty Propagation via Decoupled Multi-Probability Density Evolution Method,"This paper presents a unified framework for uncertainty propagation in
dynamical systems involving hybrid aleatory and epistemic uncertainties. The
framework accommodates precise probabilistic, imprecise probabilistic, and
non-probabilistic representations, including the distribution-free
probability-box (p-box). A central aspect of the framework involves
transforming the original uncertainty inputs into an augmented random space,
yielding the primary challenge of determining the conditional probability
density function (PDF) of the response quantity of interest given epistemic
uncertainty parameters. The recently proposed decoupled multi-probability
density evolution method (decoupled M-PDEM) is employed to numerically solve
the conditional PDF for complex dynamical systems. Several numerical examples
illustrate the applicability, efficiency, and accuracy of the proposed
framework. These include a linear single-degree-of-freedom (SDOF) system
subject to Gaussian white noise with its natural frequency modeled as a p-box,
a 10-DOF hysteretic structure subject to imprecise seismic loads, and a crash
box model with mixed random and interval system parameters.",http://arxiv.org/pdf/2509.09535v1
2509.09534v1,ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning,"Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.",http://arxiv.org/pdf/2509.09534v1
2509.09533v1,Bioluminescence tomography: A new regularized shape optimization method,"In this paper, we investigate an inverse source problem arising in
bioluminescence tomography (BLT), where the objective is to recover both the
support and intensity of the light source from boundary measurements. A shape
optimization framework is developed, in which the source strength and its
support are decoupled through first-order optimality conditions. To enhance the
stability of the reconstruction, we incorporate a parameter-dependent coupled
complex boundary method(CCBM) scheme together with perimeter and volume
regularizations. The level-set representation naturally accommodates
topological changes, enabling the reconstruction of multiple, closely located,
or nested sources. Theoretical justifications are provided, and a series of
numerical experiments are conducted to validate the proposed method. The
results demonstrate the robustness, accuracy, and noise-resistance of the
algorithm, as well as its advantages over existing approaches.",http://arxiv.org/pdf/2509.09533v1
2509.09527v1,Generative Diffusion Contrastive Network for Multi-View Clustering,"In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.",http://arxiv.org/pdf/2509.09527v1
2509.09526v1,Region-Specific Audio Tagging for Spatial Sound,"Audio tagging aims to label sound events appearing in an audio recording. In
this paper, we propose region-specific audio tagging, a new task which labels
sound events in a given region for spatial audio recorded by a microphone
array. The region can be specified as an angular space or a distance from the
microphone. We first study the performance of different combinations of
spectral, spatial, and position features. Then we extend state-of-the-art audio
tagging systems such as pre-trained audio neural networks (PANNs) and audio
spectrogram transformer (AST) to the proposed region-specific audio tagging
task. Experimental results on both the simulated and the real datasets show the
feasibility of the proposed task and the effectiveness of the proposed method.
Further experiments show that incorporating the directional features is
beneficial for omnidirectional tagging.",http://arxiv.org/pdf/2509.09526v1
2509.09524v1,DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning,"This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.",http://arxiv.org/pdf/2509.09524v1
2509.09522v1,Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs,"Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.",http://arxiv.org/pdf/2509.09522v1
2509.09521v1,Coarsening model of chromosomal crossover placement,"Chromosomal crossovers play a crucial role in meiotic cell division, as they
ensure proper chromosome segregation and increase genetic variability.
Experiments have consistently revealed two key observations across species: (i)
the number of crossovers per chromosome is typically small, but at least one,
and (ii) crossovers on the same chromosome are subject to interference, i.e.,
they are more separated than expected by chance. These observations can be
explained by a recently proposed coarsening model, where the dynamics of
droplets associated with chromosomes designate crossovers. We provide a
comprehensive analysis of the coarsening model, which we also extend by
including material exchanges between droplets, the synaptonemal complex, and
the nucleoplasm. We derive scaling laws for the crossover count, which allows
us to analyze data across species. Moreover, our model provides a coherent
explanation of experimental data across mutants, including the wild-type and
zyp1-mutant of A. thaliana. Consequently, the extended coarsening model
provides a solid framework for investigating the underlying mechanisms of
crossover placement.",http://arxiv.org/pdf/2509.09521v1
2509.09518v1,Microlocal analysis of the non-relativistic limit of the Klein--Gordon equation: Estimates,"This is the more technical half of a two-part work in which we introduce a
robust microlocal framework for analyzing the non-relativistic limit of
relativistic wave equations with time-dependent coefficients, focusing on the
Klein--Gordon equation. Two asymptotic regimes in phase space are relevant to
the non-relativistic limit: one corresponding to what physicists call
``natural'' units, in which the PDE is approximable by the free Klein--Gordon
equation, and a low-frequency regime in which the equation is approximable by
the usual Schrodinger equation. Combining the analyses in the two regimes gives
global estimates which are uniform as the speed of light goes to infinity. The
companion paper gives applications. Our main technical tools are three new
pseudodifferential calculi, $\Psi_{\natural}$ (a variant of the semiclassical
scattering calculus), $\Psi_{\natural\mathrm{res}}$, and
$\Psi_{\natural2\mathrm{res}}$, the latter two of which are created by ``second
microlocalizing'' the first at certain locations. This paper and the companion
paper can be read in either order, since the latter treats the former as a
black box.",http://arxiv.org/pdf/2509.09518v1
2509.09513v1,Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner,"The diffusion MRI Neurite Exchange Imaging model offers a promising framework
for probing gray matter microstructure by estimating parameters such as
compartment sizes, diffusivities, and inter-compartmental water exchange time.
However, existing protocols require long scan times. This study proposes a
reduced acquisition scheme for the Connectome 2.0 scanner that preserves model
accuracy while substantially shortening scan duration. We developed a
data-driven framework using explainable artificial intelligence with a guided
recursive feature elimination strategy to identify an optimal 8-feature subset
from a 15-feature protocol. The performance of this optimized protocol was
validated in vivo and benchmarked against the full acquisition and alternative
reduction strategies. Parameter accuracy, preservation of anatomical contrast,
and test-retest reproducibility were assessed. The reduced protocol yielded
parameter estimates and cortical maps comparable to the full protocol, with low
estimation errors in synthetic data and minimal impact on test-retest
variability. Compared to theory-driven and heuristic reduction schemes, the
optimized protocol demonstrated superior robustness, reducing the deviation in
water exchange time estimates by over two-fold. In conclusion, this hybrid
optimization framework enables viable imaging of neurite exchange in 14 minutes
without loss of parameter fidelity. This approach supports the broader
application of exchange-sensitive diffusion magnetic resonance imaging in
neuroscience and clinical research, and offers a generalizable method for
designing efficient acquisition protocols in biophysical parameter mapping.",http://arxiv.org/pdf/2509.09513v1
2509.09510v1,"Cognitive Affordances in Visualization: Related Constructs, Design Factors, and Framework","Classically, affordance research investigates how the shape of objects
communicates actions to potential users. Cognitive affordances, a subset of
this research, characterize how the design of objects influences cognitive
actions, such as information processing. Within visualization, cognitive
affordances inform how graphs' design decisions communicate information to
their readers. Although several related concepts exist in visualization, a
formal translation of affordance theory to visualization is still lacking. In
this paper, we review and translate affordance theory to visualization by
formalizing how cognitive affordances operate within a visualization context.
We also review common methods and terms, and compare related constructs to
cognitive affordances in visualization. Based on a synthesis of research from
psychology, human computer interaction, and visualization, we propose a
framework of cognitive affordances in visualization that enumerates design
decisions and reader characteristics that influence a visualization's hierarchy
of communicated information. Finally, we demonstrate how this framework can
guide the evaluation and redesign of visualizations.",http://arxiv.org/pdf/2509.09510v1
2509.09506v1,Frozen differential scattering in reconfigurable complex media,"The sensitivity of transmission to the input wavefront is a hallmark feature
of complex media and the basis for wavefront shaping techniques. Yet,
intriguing special cases exist in which the output wavefront is ""frozen""
(agnostic to the input wavefront). This happens when special structure in the
complex medium collapses the rank of its transmission matrix to unity. Here, we
unveil that an analogous phenomenon exists more universally for differential
scattering (including reflection) in reconfigurable complex media.
Specifically, for a localized perturbation, the differential scattering matrix
of any complex medium has rank one. One consequence is that the differential
output signal is perfectly coherent irrespective of the input wavefront's
coherence. Moreover, the thermal noise emitted into the frozen differential
output mode has a particular structure that can be exploited for thermal noise
management. We experimentally evidence frozen differential scattering in a
rich-scattering wireless link parametrized by a programmable meta-atom. Then,
we demonstrate ""customized freezing"" by optimizing the configuration of
additional programmable meta-atoms that parametrize the wireless link, as
envisioned for 6G networks. We impose particular shapes of the frozen
differential output mode, and maximize its signal-to-thermal-noise ratio.
Potential applications include filtering and stabilization of differential
wavefronts, as well as imaging, sensing, and communication in complex media.",http://arxiv.org/pdf/2509.09506v1
2509.09504v1,YSES 2b is a background star: Differential astrometric M-dwarf measurements in time,"We wish to confirm the nature of YSES 2b, a purportedly faint companion of
the young star YSES 2. We used on-sky observations from SPHERE and GRAVITY to
measure the astrometric position of 2b with respect to the star YSES 2, and
examined the competing hypotheses of (i) a bound substellar companion versus
(ii) a distant unrelated background source with a non-zero proper motion. YSES
2b appears to be a late-type M-dwarf star over 2 kiloparsecs behind the star
YSES 2. It has a transverse velocity of about 300 km/s and is located within
one of the spiral arms of the Galaxy. The main discriminant was multiple epochs
of GRAVITY astrometry that identified the sub-milliarcsecond parallactic motion
of the star.",http://arxiv.org/pdf/2509.09504v1
2509.09501v1,Region-Wise Correspondence Prediction between Manga Line Art Images,"Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.",http://arxiv.org/pdf/2509.09501v1
2509.09494v1,In-Loop Filtering Using Learned Look-Up Tables for Video Coding,"In-loop filtering (ILF) is a key technology in video coding standards to
reduce artifacts and enhance visual quality. Recently, neural network-based ILF
schemes have achieved remarkable coding gains, emerging as a powerful candidate
for next-generation video coding standards. However, the use of deep neural
networks (DNN) brings significant computational and time complexity or high
demands for dedicated hardware, making it challenging for general use. To
address this limitation, we study a practical ILF solution by adopting look-up
tables (LUTs). After training a DNN with a restricted reference range for ILF,
all possible inputs are traversed, and the output values of the DNN are cached
into LUTs. During the coding process, the filtering process is performed by
simply retrieving the filtered pixel through locating the input pixels and
interpolating between the cached values, instead of relying on heavy inference
computations. In this paper, we propose a universal LUT-based ILF framework,
termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of
filtering LUTs and propose a series of customized indexing mechanisms to enable
better filtering reference perception with limited storage consumption. Second,
we propose the cross-component indexing mechanism to enable the filtering of
different color components jointly. Third, in order to make our solution
practical for coding uses, we propose the LUT compaction scheme to enable the
LUT pruning, achieving a lower storage cost of the entire solution. The
proposed framework is implemented in the VVC reference software. Experimental
results show that the proposed framework achieves on average 0.82%/2.97%/1.63%
and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI
and RA configurations, respectively. Compared to DNN-based solutions, our
proposed solution has much lower time complexity and storage cost.",http://arxiv.org/pdf/2509.09494v1
2509.09489v1,Acoustic to Articulatory Speech Inversion for Children with Velopharyngeal Insufficiency,"Traditional clinical approaches for assessing nasality, such as
nasopharyngoscopy and nasometry, involve unpleasant experiences and are
problematic for children. Speech Inversion (SI), a noninvasive technique,
offers a promising alternative for estimating articulatory movement without the
need for physical instrumentation. In this study, an SI system trained on
nasalance data from healthy adults is augmented with source information from
electroglottography and acoustically derived F0, periodic and aperiodic energy
estimates as proxies for glottal control. This model achieves 16.92% relative
improvement in Pearson Product-Moment Correlation (PPMC) compared to a previous
SI system for nasalance estimation. To adapt the SI system for nasalance
estimation in children with Velopharyngeal Insufficiency (VPI), the model
initially trained on adult speech was fine-tuned using children with VPI data,
yielding an 7.90% relative improvement in PPMC compared to its performance
before fine-tuning.",http://arxiv.org/pdf/2509.09489v1
2509.09484v1,BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging,"Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.",http://arxiv.org/pdf/2509.09484v1
2509.09479v1,Short-term cognitive fatigue of spatial selective attention after face-to-face conversations in virtual noisy environments,"Spatial selective attention is an important asset for communication in
cocktail party situations but may be compromised by short-term cognitive
fatigue. Here we tested whether an effortful conversation in a highly
ecological setting depletes task performance in an auditory spatial selective
attention task. Young participants with normal hearing performed the task
before and after (1) having a real dyadic face-to-face conversation on a free
topic in a virtual reverberant room with simulated interfering conversations
and background babble noise at 72 dB SPL for 30 minutes, (2) passively
listening to the interfering conversations and babble noise, or (3) having the
conversation in quiet. Self-reported perceived effort and fatigue increased
after conversations in noise and passive listening relative to the reports
after conversations in quiet. In contrast to our expectations, response times
in the attention task decreased, rather than increased, after conversation in
noise and accuracy did not change systematically in any of the conditions on
the group level. Unexpectedly, we observed strong training effects between the
individual sessions in our within-subject design even after one hour of
training on a different day.",http://arxiv.org/pdf/2509.09479v1
2509.09478v1,The role of near neutron drip-line nuclei in the $r$-process,"The role of near neutron-drip-line nuclei in the rapid neutron-capture
process ($r$-process) is studied with the classical $r$-process model.
  Simulations under different astrophysical conditions ($T$, $n_n$) show that
$r$-process paths approach the neutron-drip line under low-temperature and
high-neutron-density conditions.
  A sensitivity study reveals that variations in the nuclear masses of these
exotic nuclei significantly impact the abundances of superheavy nuclei, and
lead to obvious abundance variations in the $A=110-125$, $A=175-185$, and
$A=200-205$ regions.
  By contrast, the $r$-process rare-earth peak and the $A=130,195$ peaks remain
largely unaffected.
  The nuclei that obviously impact $r$-process abundances are mainly
distributed in the region of $25\leq Z\leq 90$ and $50\leq N\leq 180$, with the
nuclei around neutron magic numbers found to be particularly important for the
$r$-process, even in the near-neutron-drip-line region.",http://arxiv.org/pdf/2509.09478v1
2509.09476v1,Bath-induced stabilization of classical non-linear response in two dimensional infrared spectroscopy,"Classical response functions have shown considerable promise in computational
2D IR modeling; however, a simple diagrammatic description, analogous to that
for open quantum systems, has been lacking. While a promising diagrammatic
approach has recently been introduced for isolated systems, the resulting
nonlinear response functions remain unstable at long times, a characteristic
feature of integrable classical systems. Here, we extend this framework to
incorporate system-bath interactions under the weak-anharmonicity approximation
and explore the resulting conditions for bath-induced stabilization. The
resulting expression for the weakly anharmonic response function is remarkably
simple and exhibits a one-to-one correspondence with the quantum counterpart in
the $\hbar\to 0$ limit, offering potential computational advantages in
extending the approach to large, multi-oscillator systems. We find that (to
lowest order in anharmonicity) the bath-induced stabilization of both linear
and nonlinear classical response functions depends sensitively on the nature of
spectral density, particularly on the balance between low-frequency and
high-frequency components. Application of this classical diagrammatic approach
to 2D IR spectroscopy of the amide I band captures the characteristic
population-time-dependent dynamics associated with spectral diffusion,
suggesting that the approach may prove useful in describing real experimental
systems at ambient temperatures.",http://arxiv.org/pdf/2509.09476v1
2509.09473v1,Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation,"The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.",http://arxiv.org/pdf/2509.09473v1
2509.09470v1,AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings,"Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.",http://arxiv.org/pdf/2509.09470v1
2509.09468v1,A Probabilistic Framework for Predicting Spatiotemporal Intensity and Variability of Outdoor Thermal Comfort,"Thermal conditions in the urban canopy exhibit stochastic variability driven
by varied radiative fluxes and turbulent wind fields, requiring probabilistic
rather than deterministic prediction methods. This study presents a
probabilistic framework for predicting the spatial and temporal intensity and
variability of outdoor thermal comfort in tropical urban environments. The
framework integrates ground-measured meteorological data and remote sensing
urban morphological data to calculate Physiological Equivalent Temperature
(PET), and applies K-means, XGBoost, and Monte Carlo simulations on PET
training and inference. The prediction model achieved strong performance, with
R2, RMSE, and SMAPE values of 0.93, 0.81 degC, and 1.34% for PET_mean, and
0.85, 0.38 degC, and 10.44% for PET_std, respectively. A case study showed
clear spatial heterogeneity of outdoor thermal comfort. Locations with dense
tree canopies and vegetated surfaces displayed a normalized percentage of
acceptable thermal comfort (NATC) up to 65%, whereas built-up zones dominated
by impervious surfaces, such as industrial estates and high-density residential
areas, recorded NATC below 30%. Greenery was found to mitigate both the
intensity of heat stress and its variability, producing a stable and
comfortable microclimate. Daytime PET_std ranged from 4.0-4.5 degC in built-up
areas to 1.5-2.0 degC in greenery-covered zones, while nighttime PET_std
decreased to 2.2-2.4 degC and 1.2-1.4 degC, respectively. These findings
emphasize the critical role of greenery in mitigating thermal variability and
enhancing outdoor thermal comfort, while revealing the stochastic nature of
thermal comfort across different urban morphologies.",http://arxiv.org/pdf/2509.09468v1
2509.09467v1,"Inteligencia Artificial jurdica y el desafo de la veracidad: anlisis de alucinaciones, optimizacin de RAG y principios para una integracin responsable","This technical report analyzes the challenge of ""hallucinations"" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a ""consultative"" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las ""alucinaciones""
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA ""consultiva"" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.",http://arxiv.org/pdf/2509.09467v1
2509.09465v1,Enhancing Optical Imaging via Quantum Computation,"Extracting information from weak optical signals is a critical challenge
across a broad range of technologies. Conventional imaging techniques,
constrained to integrating over detected signals and classical post-processing,
are limited in signal-to-noise ratio (SNR) from shot noise accumulation in the
post-processing algorithms. We show that these limitations can be circumvented
by coherently encoding photonic amplitude information into qubit registers and
applying quantum algorithms to process the stored information from
asynchronously arriving optical signals. As a specific example, we develop a
quantum algorithm for imaging unresolved point sources and apply it to
exoplanet detection. We demonstrate that orders-of-magnitude improvements in
performance can be achieved under realistic imaging conditions using relatively
small scale quantum processors.",http://arxiv.org/pdf/2509.09465v1
2509.09461v1,Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries with Human Intervention,"We propose leveraging Large Language Models (LLMs) as an interaction layer
for medical visualization systems. In domains like healthcare, where users must
navigate high-dimensional, coded, and heterogeneous datasets, LLM-generated
queries enable expert medical users to express complex analytical intents in
natural language. These intents are then translated into editable and
executable queries, replacing the dynamic query interfaces used by traditional
visualization systems built around sliders, check boxes, and drop-downs. This
interaction model reduces visual clutter and eliminates the need for users to
memorize field names or system codes, supporting fluid exploration, with the
drawback of not exposing all the filtering criteria. We also reintroduce
dynamic queries on demand to better support interactive exploration. We posit
that medical users are trained to know the possible filtering options but
challenged to remember the details of the attribute names and code values. We
demonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired
patient care pathway visualization system powered by the French National Health
Data System, one of the largest health data repositories in the world.",http://arxiv.org/pdf/2509.09461v1
2509.09459v1,Boosting Data Utilization for Multilingual Dense Retrieval,"Multilingual dense retrieval aims to retrieve relevant documents across
different languages based on a unified retriever model. The challenge lies in
aligning representations of different languages in a shared vector space. The
common practice is to fine-tune the dense retriever via contrastive learning,
whose effectiveness highly relies on the quality of the negative sample and the
efficacy of mini-batch data. Different from the existing studies that focus on
developing sophisticated model architecture, we propose a method to boost data
utilization for multilingual dense retrieval by obtaining high-quality hard
negative samples and effective mini-batch data. The extensive experimental
results on a multilingual retrieval benchmark, MIRACL, with 16 languages
demonstrate the effectiveness of our method by outperforming several existing
strong baselines.",http://arxiv.org/pdf/2509.09459v1
2509.09456v1,FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model,"Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.",http://arxiv.org/pdf/2509.09456v1
2509.09453v1,"Toward quantum-safe scalable networks: an open, standards-aware key management framework","With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.",http://arxiv.org/pdf/2509.09453v1
2509.09448v1,TORSO: Template-Oriented Reasoning Towards General Tasks,"The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.",http://arxiv.org/pdf/2509.09448v1
2509.09445v1,Photonic Matrix Multiplication Circuit Based on Double Racetrack Resonator Building Blocks,"This paper presents a novel design framework for photonic matrix
multiplication based on programmable photonic integrated circuits using double
racetrack (DRT) resonators as building blocks. Here, we analytically
demonstrate that the transfer function of the DRT resonator building block
resembles that conventional building blocks, such as directional couplers and
MZI, making it suitable for building programmable circuits that handle complex
matrix calculations. Using this new DRT resonators building block, a 3-by-3
photonic processor is implemented and validated through full-wave Finite
Element Method (FEM) simulations, and scalability is further analysed using
hybrid FEM-circuit modelling. Additionally, we implement a low-pass filter as a
non-unitary system example, showcasing the flexibility of the approach. Results
confirm high fidelity between simulated and analytical models, supporting the
viability of DRT resonators for reconfigurable photonic circuits. We believe
that the proposed DRT resonator building blocks have the potential to
complement and integrate with other previously reported blocks, thereby
enhancing the fidelity and expanding the application scope of programmable
photonic integrated circuits, particularly for all-optical signal processing in
communication systems and for integration within microwave photonics platforms
targeting emerging telecommunications technologies.",http://arxiv.org/pdf/2509.09445v1
2509.09444v1,Steady advection-diffusion in multiply-connected potential flows,"We consider the steady heat transfer between a collection of impermeable
obstacles immersed in an incompressible 2D potential flow, when each obstacle
has a prescribed boundary temperature distribution. Inside the fluid, the
temperature satisfies a variable-coefficient elliptic partial differential
equation (PDE), the solution of which usually requires expensive techniques. To
solve this problem efficiently, we construct multiply-connected conformal maps
under which both the domain and governing equation are greatly simplified. In
particular, each obstacle is mapped to a horizontal slit and the governing
equation becomes a constant-coefficient elliptic PDE. We then develop a
boundary integral approach in the mapped domain to solve for the temperature
field when arbitrary Dirichlet temperature data is specified on the obstacles.
The inverse conformal map is then used to compute the temperature field in the
physical domain. We construct our multiply-connected conformal maps by
exploiting the flexible and highly accurate AAA-LS algorithm. In
multiply-connected domains and domains with non-constant boundary temperature
data, we note similarities and key differences in the temperature fields and
Nusselt number scalings as compared to the isothermal simply-connected problem
analyzed by Choi et al. (2005). In particular, we derive new asymptotic
expressions for the Nusselt number in the case of arbitrary non-constant
temperature data in singly connected domains at low P\'eclet number, and verify
these scalings numerically. While our language focuses on the problem of
conjugate heat transfer, our methods and findings are equally applicable to the
advection-diffusion of any passive scalar in a potential flow.",http://arxiv.org/pdf/2509.09444v1
2509.09440v1,Let's Simply Count: Quantifying Distributional Similarity Between Activities in Event Data,"To obtain insights from event data, advanced process mining methods assess
the similarity of activities to incorporate their semantic relations into the
analysis. Here, distributional similarity that captures similarity from
activity co-occurrences is commonly employed. However, existing work for
distributional similarity in process mining adopt neural network-based
approaches as developed for natural language processing, e.g., word2vec and
autoencoders. While these approaches have been shown to be effective, their
downsides are high computational costs and limited interpretability of the
learned representations.
  In this work, we argue for simplicity in the modeling of distributional
similarity of activities. We introduce count-based embeddings that avoid a
complex training process and offer a direct interpretable representation. To
underpin our call for simple embeddings, we contribute a comprehensive
benchmarking framework, which includes means to assess the intrinsic quality of
embeddings, their performance in downstream applications, and their
computational efficiency. In experiments that compare against the state of the
art, we demonstrate that count-based embeddings provide a highly effective and
efficient basis for distributional similarity between activities in event data.",http://arxiv.org/pdf/2509.09440v1
2509.09439v1,Fork: Supporting POSIX fork Within a Single-Address-Space OS,"Single-address-space operating systems have well-known lightweightness
benefits that result from their central design idea: the kernel and
applications share a unique address space. This model makes these operating
systems (OSes) incompatible by design with a large class of software:
multiprocess POSIX applications. Indeed, the semantics of the primitive used to
create POSIX processes, fork, are inextricably tied to the existence of
multiple address spaces.
  Prior approaches addressing this issue trade off lightweightness,
compatibility and/or isolation. We propose {\mu}Fork, a single-address-space
operating system design supporting POSIX fork on modern hardware without
compromising on any of these key objectives. {\mu}Fork emulates POSIX processes
({\mu}processes) and achieves fork by creating for the child a copy of the
parent {\mu}process' memory at a different location within a single address
space. This approach presents two challenges: relocating the child's absolute
memory references (pointers), as well as providing user/kernel and
{\mu}processes isolation without impacting lightweightness. We address them
using CHERI. We implement {\mu}Fork and evaluate it upon three real-world
use-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS
worker warm-up. {\mu}Fork outperforms previous work and traditional monolithic
OSes on key lightweightness metrics by an order of magnitude, e.g. it can offer
a fork-bound FaaS function throughput 24% higher than that of a monolithic OS,
and can fork a {\mu}process in 54{\mu}s, 3.7x faster than a traditional fork.",http://arxiv.org/pdf/2509.09439v1
2509.09438v1,GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models,"Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.",http://arxiv.org/pdf/2509.09438v1
2509.09436v1,Influence of dissolved gas concentration on the lifetime of surface bubbles in volatile liquids,"Bubbles at the air-liquid interface are important for many natural and
industrial processes. Factors influencing the lifetime of such surface bubbles
have been investigated extensively, yet the impact of dissolved gas
concentration remains unexplored. Here we investigate how the lifetime of
surface bubbles in volatile liquids depends on the dissolved gas concentration.
The bubble lifetime is found to decrease with the dissolved gas concentration.
Larger microbubbles at increased gas concentration are found to trigger bubble
bursting at earlier times. Combined with the thinning rate of the bubble cap
thickness, a scaling law of the bubble lifetime is developed. Our findings may
provide new insight on bubble and foam stability.",http://arxiv.org/pdf/2509.09436v1
2509.09435v1,Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing,"Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.",http://arxiv.org/pdf/2509.09435v1
2509.09433v1,Long-term operation of the screen-printed graphite-based resistive coatings on the HPL electrode for the Resistive Plate Chamber,"The reliability of large-area Resistive Plate Chambers (RPCs) operated under
High-Luminosity Large Hadron Collider (HL-LHC) conditions is governed by the
long-term stability and radiation tolerance of screen-printed graphite/phenoxy
coatings on high-pressure-laminate (HPL) electrodes. This work presents a
comprehensive, end-to-end qualification of such coatings that integrates
industrial process control and metrology with controlled humidity/temperature
campaigns, extended high-voltage stress testing to decade-scale charge levels,
and representative neutron and gamma irradiation at CERN facilities. The
results establish reproducible industrial coating production, stable
performance under sustained operation and irradiation, and practical acceptance
criteria with operating and monitoring guidelines. The study provides a
transferable quality-assurance framework for graphite-based resistive coatings
on HPL electrodes, enabling reproducible production and reliable RPC
performance for the HL-LHC upgrades and for future high-rate collider
experiments.",http://arxiv.org/pdf/2509.09433v1
2509.09428v1,Upper triangular matrices with superinvolution: identities and images of multilinear polynomials,"In this paper we consider the algebra of upper triangular matrices UT$_n(F)$,
endowed with a $\mathbb{Z}_2$-grading (superalgebra) and equipped with a
superinvolution. These structures naturally arise in the context of Lie and
Jordan superalgebras and play a central role in the theory of polynomial
identities with involution, as showed in the framework developed by Aljadeff,
Giambruno, and Karasik in [2]. We provide a complete description of the
identities of UT$_4(F)$, where the grading is induced by the sequence
$(0,1,0,1)$ and the superinvolution is the super-symplectic one. This work
extends previous classifications obtained for the cases $n = 2$ and $n = 3$,
and addresses an open problem for $n \geq 4$. In the last part of the paper, we
investigate the image of multilinear polynomials on the superalgebra UT$_n(F)$
with superinvolution, showing that the image is a vector space if and only if
$n \leq 3$, thus contributing to an analogue of the L'vov-Kaplansky conjecture
in this context.",http://arxiv.org/pdf/2509.09428v1
2509.09427v1,FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution,"As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.",http://arxiv.org/pdf/2509.09427v1
2509.09424v1,ENSI: Efficient Non-Interactive Secure Inference for Large Language Models,"Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.",http://arxiv.org/pdf/2509.09424v1
2509.09422v1,A Comparative Analysis of Robust and Reliable Designs Using the Compromised Design Support Problem: A Case Study in Hot Rod Rolling Processes,"Design under uncertainty is a challenging problem, as a systems performance
can be highly sensitive to variations in input parameters and model
uncertainty. A conventional approach to addressing such problems is robust
optimization, which seeks to enhance design performance by reducing sensitivity
to uncertainty. Alternatively, reliability-based design focuses on optimizing
performance while ensuring that failure constraints are satisfied with a
specified probability. While both methods are well established, their
integration into multi-objective and multi-stakeholder decision-making
frameworks remains a challenging problem. In this study, we extend the
Compromise Decision Support Problem (cDSP) framework to incorporate
reliability-based design considerations and evaluate its performance in
comparison to the conventional robust-based cDSP formulation. The developed
framework has been validated on a multidisciplinary hot rod rolling process
including parametric and model uncertainties. The results compare the predicted
performance under robust and reliable scenarios, validating the efficiency of
the approach in managing uncertainties for complex, multidisciplinary systems.
Specifically, we found that the two methods exhibit markedly different
performance when the predicted performance follows a non-normal distribution, a
situation that arises in non-linear systems with parametric uncertainty. Based
on this insight, we offer guidance to designers on the conditions under which
each method is most appropriate.",http://arxiv.org/pdf/2509.09422v1
2509.09421v1,Attributed-graphs kernel implementation using local detuning of neutral-atoms Rydberg Hamiltonian,"We extend the quantum-feature kernel framework, which relies on measurements
of graph-dependent observables, along three directions. First, leveraging
neutral-atom quantum processing units (QPUs), we introduce a scheme that
incorporates attributed graphs by embedding edge features into atomic positions
and node features into local detuning fields of a Rydberg Hamiltonian. We
demonstrate both theoretically and empirically that local detuning enhances
kernel expressiveness. Second, in addition to the existing quantum evolution
kernel (QEK), which uses global observables, we propose the
generalized-distance quantum-correlation (GDQC) kernel, based on local
observables. While the two kernels show comparable performance, we show that
GDQC can achieve higher expressiveness. Third, instead of restricting to
observables at single time steps, we combine information from multiple stages
of the quantum evolution via pooling operations. Using extensive simulations on
two molecular benchmark datasets, MUTAG and PTC\_FM, we find: (a) QEK and GDQC
perform competitively with leading classical algorithms; and (b) pooling
further improves performance, enabling quantum-feature kernels to surpass
classical baselines. These results show that node-feature embedding and kernel
designs based on local observables advance quantum-enhanced graph machine
learning on neutral-atom devices.",http://arxiv.org/pdf/2509.09421v1
2509.09420v1,HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing,"Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.",http://arxiv.org/pdf/2509.09420v1
2509.09419v1,Impact of rotation on the accretion of entropy perturbations in collapsing massive stars,"Convection in the innermost shells of massive stars plays an important role
in initiating core-collapse supernovae. When these convective motions reach the
supernova shock, they create extra turbulence, which helps energize the
explosion. In our earlier work, we studied the effect of rotation on the
hydrodynamic evolution of convective vortices in collapsing stars. This study
focuses on how rotation influences the entropy perturbations, which naturally
form in turbulent convection. As these perturbations are carried inward with
the collapsing star, they generate both vorticity and sound waves. Using linear
perturbation theory, we model entropy waves as small disturbances on top of a
steady background flow. Our results show that stellar rotation has little
effect on the evolution of entropy perturbations during collapse, prior to
encountering the supernova shock. This outcome is consistent with our earlier
findings on the limited influence of rotation in the accretion of convective
eddies.",http://arxiv.org/pdf/2509.09419v1
2509.09417v1,"LMC+: Large-scale mapping of [CII] and [OIII] in the LMC molecular ridge, I. Dataset and line ratio analyses","The fundamental process of star formation in galaxies involves the interplay
between the fueling of star formation via molecular gas and the feedback from
recently formed massive stars. This process, by which galaxies evolve, is also
closely connected to the intrinsic properties of the interstellar medium (ISM).
To study the role that different molecular and atomic phases of the ISM play in
star formation, and to characterize their physical conditions, we zoom into our
nearest neighboring galaxy, the Large Magellanic Cloud (LMC; 50 kpc). The LMC
offers a view of the ISM and star formation conditions in a low metallicity
environment similar to, in that regard, the epoch of the peak of star formation
in the earlier universe. We present an unprecedentedly detailed analysis of a
well-known star-forming regions (SFRs) at a spatial resolution of a few pc. We
mapped a 610pcx260pc region in the LMC molecular ridge in [CII] and the [OIII]
using the FIFI-LS instrument on the SOFIA telescope. We compare the data with
the distribution of the CO (2-1) emission from ALMA, the modeled TIR luminosity
as well as Spitzer/MIPS continuum and Halpha. We also provide a detailed
description of the observing strategy and the data reduction. We find that
[CII] and [OIII] emission is associated with the SFRs in the molecular ridge,
but also extends throughout the mapped region, not obviously associated with
ongoing star formation. The CO emission is clumpier than the [CII] emission and
we find plentiful [CII] present where there is little CO emission, possibly
holding important implications for CO-dark gas. We find a clear trend of the
[CII]/TIR ratio decreasing with increasing TIR. This suggests a strong link
between the [CII]-deficit and the local physical conditions instead of global
properties.",http://arxiv.org/pdf/2509.09417v1
2509.09413v1,Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples,"Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.",http://arxiv.org/pdf/2509.09413v1
2509.09401v1,Moduli spaces of open strings have polylogarithmic Mirzakhani volumes,"We show that the Mirzakhani volume, as introduced by Chekhov, of the moduli
space of every non-contractible crowned hyperbolic surface is naturally
expressible as a sum of Gaussian rational multiples of polylogarithms evaluated
at $\pm1$ and $\pm\sqrt{-1}$.",http://arxiv.org/pdf/2509.09401v1
2509.09397v1,Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift,"Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.",http://arxiv.org/pdf/2509.09397v1
2509.09396v1,LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations,"To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.",http://arxiv.org/pdf/2509.09396v1
2509.09390v1,Scanning photocurrent microscopy and its application to one- and two-dimensional materials,"The electrical response of a material when illuminated with light is a key to
many optoelectronic device applications. This so-called photoresponse typically
has a non-uniform spatial distribution through the active device area, and the
ability to spatially resolve the photoresponse enables an in-depth
understanding of the underlying physical mechanisms. Scanning photocurrent
microscopy (SPCM) is a method that allows the spatial mapping of the
photoresponse by raster scanning a focused laser beam over the sample. SPCM is
becoming more popular due to its simplicity and power in unraveling fundamental
optoelectronic processes. In this review, first, we provide the fundamentals of
SPCM to lay the basics for the subsequent discussions. Then, we focus on the
literature that employs SPCM to identify the photoresponse of one- and
two-dimensional materials. We discuss SPCM measurement results of common
materials in detail and introduce a systematic approach to interpreting the
SPCM measurements. We have given particular emphasis on the photothermal
mechanisms that are excited by the focused laser beam and critically reviewed
studies in the literature from the perspective of laser-induced heating of the
electronic and the lattice degrees of freedom. Finally, we discuss the
shortcomings of SPCM in determining the mechanisms leading to the
photoresponse.",http://arxiv.org/pdf/2509.09390v1
2509.09388v1,Hierarchical Bracketing Encodings Work for Dependency Graphs,"We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.",http://arxiv.org/pdf/2509.09388v1
2509.09387v1,MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization,"Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.",http://arxiv.org/pdf/2509.09387v1
2509.09381v1,Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research,"Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.",http://arxiv.org/pdf/2509.09381v1
2509.09376v1,Scaling approach to rigid and soft nuclear deformation through flow fluctuations in high-energy nuclear collisions,"The nature of octupole deformation, whether static or vibrational, remains an
open question in nuclear physics. Here, we propose a scaling approach to probe
this ambiguity by triangular flow fluctuations using multi-particle cumulants,
$c_{3,\varepsilon}\{4\}$, in relativistic $^{238}$U+$^{238}$U collisions. We
demonstrate that both $|c_{3,\varepsilon}\{4\}|$ and the ratio
$|c_{3,\varepsilon}\{4\}/c^2_{3,\varepsilon}\{2\}|$ scale linearly with the
fourth-order moment of octupole deformation, $\langle \beta^4_{3,\mathrm{U}}
\rangle$. Combined with the known linear relation of $c_{3,\varepsilon}\{2\}$
to $\langle \beta^2_{3,\mathrm{U}} \rangle$, this new relation provides a
direct extraction of both the mean and variance of the octupole deformation
fluctuations, finally discriminating between static and dynamic origins. This
work establishes a new tool to probe the static and dynamic collective modes in
high-energy nuclear collisions, advancing a significant step toward refining
the initial conditions of quark-gluon plasma.",http://arxiv.org/pdf/2509.09376v1
2509.09375v1,Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality,"Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.",http://arxiv.org/pdf/2509.09375v1
2509.09374v1,Diabatic quantum annealing for training energy-based generative models,"Energy-based generative models, such as restricted Boltzmann machines (RBMs),
require unbiased Boltzmann samples for effective training. Classical Markov
chain Monte Carlo methods, however, converge slowly and yield correlated
samples, making large-scale training difficult. We address this bottleneck by
applying the analytic relation between annealing schedules and effective
inverse temperature in diabatic quantum annealing. By implementing this
prescription on a quantum annealer, we obtain temperature-controlled Boltzmann
samples that enable RBM training with faster convergence and lower validation
error than classical sampling. We further identify a systematic temperature
misalignment intrinsic to analog quantum computers and propose an analytical
rescaling method that mitigates this hardware noise, thereby enhancing the
practicality of quantum annealers as Boltzmann samplers. In our method, the
model's connectivity is set directly by the qubit connectivity, transforming
the computational complexity inherent in classical sampling into a requirement
on quantum hardware. This shift allows the approach to extend naturally from
RBMs to fully connected Boltzmann machines, opening opportunities inaccessible
to classical training methods.",http://arxiv.org/pdf/2509.09374v1
2509.09373v1,Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems,"Pixel-based fluid antennas provide enhanced multiplexing gains and quicker
radiation pattern switching than traditional designs. However, this innovation
introduces challenges for channel estimation and analog precoding due to the
state-non-separable channel response problem. This paper explores a multiuser
MIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements
from a real-world prototype. We present a sparse channel recovery framework for
uplink channel sounding, employing an approximate separable channel response
model with DNN-based antenna radiation functions. We then propose two
low-complexity channel estimation algorithms that leverage orthogonal matching
pursuit and variational Bayesian inference to accurately recover channel
responses across various scattering cluster angles. These estimations enable
the prediction of composite channels for all fluid antenna states, leading to
an analog precoding scheme that optimally selects switching states for
different antennas. Our simulation results indicate that the proposed approach
significantly outperforms several baseline methods, especially in high
signal-to-noise ratio environments with numerous users.",http://arxiv.org/pdf/2509.09373v1
2509.09372v1,VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model,"Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.",http://arxiv.org/pdf/2509.09372v1
2509.09370v1,Capillary Rise in Pipes with Variable Cross Section,"This work proposes an ODE model for a capillary rise in pipes with variable
cross section and compares it to the lubrication theory model. Two key
assumptions are made: (1) radius of the pipe varies with axial coordinate, and
(2) pipe's convergence angle is small. The model reduction process involves the
identification of critical parameters and simplifies the governing equations by
neglecting higher-order terms. Under appropriate scaling, it is shown that
generalized Washburn's equation for capillary rise in pipes with variable cross
section reduces to the known lubrication theory model.",http://arxiv.org/pdf/2509.09370v1
2509.09369v1,Impact of Force Noise on the Visibility of STA-Based Atom Interferometers,"In a previous work, we designed a compact atom interferometer to measure
homogeneous constant forces guiding the arms via shortcuts to adiabatic paths.
Within this scheme we drive the atom by moving spin-dependent traps, and design
a force $f(t)$ to compensate inertial terms in the moving frame. In this paper
we analyze how robust our interferometer is against some realistic noisy
deviation from the unperturbed value of the force $f(t)$. The complex overlap
of the atom wave functions for each arm of the interferometer at the final time
of the process is calculated. We demonstrate that the measure of the unknown
force will not be affected, as there will be no contribution of the error in
the phase difference of the overlap. Nevertheless, the visibility will be
reduced as the modulus will no longer be one. In addition we find the optimal
trajectories minimizing the effect of the noise in the visibility while keeping
the sensitivity of the interferometer.",http://arxiv.org/pdf/2509.09369v1
2509.09368v1,"A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data","Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.",http://arxiv.org/pdf/2509.09368v1
2509.09365v1,Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection,"We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.",http://arxiv.org/pdf/2509.09365v1
2509.09360v1,MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems,"Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.",http://arxiv.org/pdf/2509.09360v1
2509.09357v1,Enhancing Oxygen Reduction Reaction on Pt-Based Electrocatalysts through Surface Decoration for Improved OH Reduction Equilibrium and Reduced H2O Adsorption,"Electrochemical energy and substance conversion devices involve complex
electrode processes, characterized by multiple charge transfer steps, competing
pathways, and various intermediates. Such complexity makes it challenging to
enhance electrocatalytic activity. The prevailing strategy typically focuses on
optimizing the geometric and electronic structures of the electrocatalysts to
align the adsorption energies of reaction intermediates with the peak of the
activity Volcano curve. In this study, we demonstrate that surface decoration
can effectively shape the micro reaction environment for the model system of
oxygen reduction reaction (ORR) on Pt electrodes. By applying a partial
hydrophobic I* adlayer on the Pt surface, we can shift the equilibrium of OH*
reduction and weaken H2O* adsorption, which significantly enhances ORR
kinetics. With in situ scan tunneling microscopy (STM) and theoretical
calculations, our study reveals the formation of isolated Pt2 surface units
situated in a hydrophobic valley surrounded by adsorbed iodine atoms. This
minimalist Pt2 active unit exhibits significantly greater activity for ORR
compared to an extended Pt surface. This strategy could pave the way for
developing highly efficient catalysts with potential applications in fuel cell
technology and metal air batteries and extension to other electrochemical
conversion reactions such as ammonia synthesis and CO2 reduction.",http://arxiv.org/pdf/2509.09357v1
2509.09356v1,Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning,"Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.",http://arxiv.org/pdf/2509.09356v1
2509.09355v1,The History of Galaxy Mergers in IllustrisTNG,"The process of galaxy evolution over cosmic time is not yet fully understood,
since there is a debate on the impact of galaxy collisions on the star
formation and metallicity. The local environment of the galaxy mergers could
also have a large impact on the evolution of the galaxies, but it has not yet
been possible to examine it in detail. Modern simulations with larger capacity,
including the newest physical knowledge and new observations with JWST, help us
to answer these questions. Using the IllustrisTNG cosmological simulation, we
processed the catalogue data and the merger tree files of the TNG300-1
simulation. We calculated the galaxies average star formation rate (SFR) and
mass at redshifts between 0 < z < 15. We investigated the environment of galaxy
mergers, with the focus on the local density, and also examined how the SFR
changes in merging galaxies. We compared our findings with JWST results and
highlighted differences in the star formation rate density (SFRD) history
between the models and observations.",http://arxiv.org/pdf/2509.09355v1
2509.09349v1,Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles,"Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.",http://arxiv.org/pdf/2509.09349v1
2509.09344v1,Lifetime of bimerons and antibimerons in two-dimensional magnets,"Soliton-based computing architectures have recently emerged as a promising
avenue to overcome fundamental limitations of conventional information
technologies, the von Neumann bottleneck. In this context, magnetic skyrmions
have been widely considered for in-situ processing devices due to their
mobility and enhanced lifetime in materials with broken inversion symmetry.
However, modern applications in non-volatile reservoir or neuromorphic
computing raise the additional demand for non-linear inter-soliton
interactions. Here we report that solitons in easy-plane magnets, such as
bimerons and antibimerons, show greater versatility and potential for
non-linear interactions than skyrmions and antiskyrmions, making them superior
candidates for this class of applications. Using first-principles and
transition state theory, we predict the coexistence of degenerate bimerons and
antibimerons at zero field in a van der Waals heterostructure
Fe$_3$GeTe$_2$/Cr$_2$Ge$_2$Te$_6$ -- an experimentally feasible system. We
demonstrate that, owing to their distinct structural symmetry, bimerons exhibit
fundamentally different behavior from skyrmions and cannot be regarded as their
in-plane counterparts, as is often assumed. This distinction leads to unique
properties of bimerons and antibimerons, which arise from the unbroken
rotational symmetry in easy-plane magnets. These range from anisotropic
soliton-soliton interactions to strong entropic effects on their lifetime,
driven by the non-local nature of thermal excitations. Our findings reveal a
broader richness of solitons in easy-plane magnets and underline their unique
potential for spintronic devices.",http://arxiv.org/pdf/2509.09344v1
2509.09343v1,Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments,"Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.",http://arxiv.org/pdf/2509.09343v1
2509.09342v1,CESRec: Constructing Pseudo Interactions for Sequential Recommendation via Conversational Feedback,"Sequential Recommendation Systems (SRS) have become essential in many
real-world applications. However, existing SRS methods often rely on
collaborative filtering signals and fail to capture real-time user preferences,
while Conversational Recommendation Systems (CRS) excel at eliciting immediate
interests through natural language interactions but neglect historical
behavior. To bridge this gap, we propose CESRec, a novel framework that
integrates the long-term preference modeling of SRS with the real-time
preference elicitation of CRS. We introduce semantic-based pseudo interaction
construction, which dynamically updates users'historical interaction sequences
by analyzing conversational feedback, generating a pseudo-interaction sequence
that seamlessly combines long-term and real-time preferences. Additionally, we
reduce the impact of outliers in historical items that deviate from users'core
preferences by proposing dual alignment outlier items masking, which identifies
and masks such items using semantic-collaborative aligned representations.
Extensive experiments demonstrate that CESRec achieves state-of-the-art
performance by boosting strong SRS models, validating its effectiveness in
integrating conversational feedback into SRS.",http://arxiv.org/pdf/2509.09342v1
2509.09339v1,Electrophoretic Beamforming in Molecular Communication: Toward Targeted Extracellular Vesicle Delivery,"Directing extracellular vesicles (EVs), such as exosomes and microvesicles,
toward specific cells is an emerging focus in nanomedicine, owing to their
natural role as carriers of proteins, RNAs, and drugs. EVs can be manipulated
by external electric fields due to their intrinsic surface charge and
biophysical properties. This study investigates the feasibility of using
extremely low-frequency electromagnetic fields to guide EV transport. A
theoretical framework based on the Fokker-Planck equation was developed and
numerically solved to model vesicle trajectories under time-harmonic drift.
Computational simulations were conducted to systematically assess the influence
of key electric field parameters, including phase, frequency, and intensity, on
vesicle displacement and trajectory. The findings demonstrate that frequencies
below 5 Hz combined with field strengths of 200-2000 V/m can induce substantial
directional control of EV motion. Moreover, enhanced directivity was achieved
through the application of multi-component electric fields. Overall, this work
establishes a theoretical foundation for the external-field-based beamforming
of nanoparticles within the framework of molecular communication.",http://arxiv.org/pdf/2509.09339v1
2509.09333v1,Toward Precise Curve Offsetting Constrained to Parametric Surfaces,"Computing offsets of curves on parametric surfaces is a fundamental yet
challenging operation in computer aided design and manufacturing. Traditional
analytical approaches suffer from time-consuming geodesic distance queries and
complex self intersection handling, while discrete methods often struggle with
precision. In this paper, we propose a totally different algorithm paradigm.
Our key insight is that by representing the source curve as a sequence of line
segment primitives, the Voronoi decomposition constrained to the parametric
surface enables localized offset computation. Specifically, the offsetting
process can be efficiently traced by independently visiting the corresponding
Voronoi cells. To address the challenge of computing the Voronoi decomposition
on parametric surfaces, we introduce two key techniques. First, we employ
intrinsic triangulation in the parameter space to accurately capture geodesic
distances. Second, instead of directly computing the surface-constrained
Voronoi decomposition, we decompose the triangulated parameter plane using a
series of plane cutting operations. Experimental results demonstrate that our
algorithm achieves superior accuracy and runtime performance compared to
existing methods. We also present several practical applications enabled by our
approach.",http://arxiv.org/pdf/2509.09333v1
2509.09332v1,OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning,"Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io",http://arxiv.org/pdf/2509.09332v1
2509.09329v1,Automated selection of nuclear coordinates for reduced dimensionality nonadiabatic dynamics,"Poor scaling of dynamics simulations with number of dimensions is currently a
major limiting factor in the simulation of photochemical processes. In this
work, we investigate ways to reduce the dimensionality of many-atom systems
with a view toward enhancing computational efficiency while maintaining
accuracy. Using mixed quantum-classical Trajectory Surface Hopping (TSH)
simulations of three photoreactive molecules - trans-azomethane (tAZM),
butyrolactone (Bulac), and furanone (Fur) - we explore two different
dimensionality reduction techniques: Principal Component Analysis (PCA) and
Normal Mode Variance (NMV). Dynamics simulations are run in full dimensionality
and reduced dimensionality, employing either PCA or NMV, and the impact of the
dimensionality reduction on selected electronic and geometric properties of the
dynamics is evaluated. For all three molecules, both PCA and NMV can be used to
select lower-dimensional spaces in which the full-dimensionality dynamics
results are reproduced. PCA reduction outperforms NMV in all systems, allowing
for a more significant dimensionality reduction without loss of accuracy. The
improved accuracy of PCA is, for tAZM, mostly seen in the electronic properties
while for both Fur and Bulac the advantage is clear in the ring-opening
reaction itself as well. The present approach opens routes to simulation of
larger photochemically relevant systems, through the use of automated
dimensionality reduction, avoiding human bias.",http://arxiv.org/pdf/2509.09329v1
2509.09322v1,ORCA: Unveiling Obscure Containers In The Wild,"Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.",http://arxiv.org/pdf/2509.09322v1
2509.09321v1,"Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization","Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.",http://arxiv.org/pdf/2509.09321v1
2509.09320v1,Quantum Coherence and Anomalous Work Extraction in Qubit Gate Dynamics,"We develop a framework based on the Kirkwood-Dirac quasiprobability
distribution to quantify the contribution of coherence to work extraction
during generic, cyclic quantum evolutions. In particular, we focus on
``anomalous processes'', counterintuitive scenarios in which, due to the
negativity of the quasiprobability distribution, work can be extracted, even
when individual processes are associated with energy gain. Applying this
framework to qubits undergoing sequences of single- and two-qubit gate
operations, we identify specific conditions under which such anomalous work
exchanges occur. Furthermore, we analyze the quasiprobabilistic structure of
deep quantum circuits and establish a compositional relation linking the work
statistics of full circuits to those of their constituent gates. Our work
highlights the role of coherence in the thermodynamics of quantum computation
and provides a foundation for systematically studying potential thermodynamic
relevance of specific quantum circuits.",http://arxiv.org/pdf/2509.09320v1
2509.09318v1,Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms,"This paper investigates automatic piano transcription based on
computationally-efficient yet high-performant variants of the Transformer that
can capture longer-term dependency over the whole musical piece. Recently,
transformer-based sequence-to-sequence models have demonstrated excellent
performance in piano transcription. These models, however, fail to deal with
the whole piece at once due to the quadratic complexity of the self-attention
mechanism, and music signals are thus typically processed in a sliding-window
manner in practice. To overcome this limitation, we propose an efficient
architecture with sparse attention mechanisms. Specifically, we introduce
sliding-window self-attention mechanisms for both the encoder and decoder, and
a hybrid global-local cross-attention mechanism that attends to various spans
according to the MIDI token types. We also use a hierarchical pooling strategy
between the encoder and decoder to further reduce computational load. Our
experiments on the MAESTRO dataset showed that the proposed model achieved a
significant reduction in computational cost and memory usage, accelerating
inference speed, while maintaining transcription performance comparable to the
full-attention baseline. This allows for training with longer audio contexts on
the same hardware, demonstrating the viability of sparse attention for building
efficient and high-performance piano transcription systems. The code is
available at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.",http://arxiv.org/pdf/2509.09318v1
2509.09313v1,Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data,"Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.",http://arxiv.org/pdf/2509.09313v1
2509.09311v1,Image Recognition with Vision and Language Embeddings of VLMs,"Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.",http://arxiv.org/pdf/2509.09311v1
2509.09307v1,Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization,"Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.",http://arxiv.org/pdf/2509.09307v1
2509.09306v1,"Listening for ""You"": Enhancing Speech Image Retrieval via Target Speaker Extraction","Image retrieval using spoken language cues has emerged as a promising
direction in multimodal perception, yet leveraging speech in multi-speaker
scenarios remains challenging. We propose a novel Target Speaker Speech-Image
Retrieval task and a framework that learns the relationship between images and
multi-speaker speech signals in the presence of a target speaker. Our method
integrates pre-trained self-supervised audio encoders with vision models via
target speaker-aware contrastive learning, conditioned on a Target Speaker
Extraction and Retrieval module. This enables the system to extract spoken
commands from the target speaker and align them with corresponding images.
Experiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantly
outperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3
speaker scenarios, respectively - substantial improvements over single speaker
baselines and state-of-the-art models. Our approach demonstrates potential for
real-world deployment in assistive robotics and multimodal interaction systems.",http://arxiv.org/pdf/2509.09306v1
2509.09305v1,Dust growth and planet formation by disc fragmentation,"It is often argued that gravitational instability of realistic protoplanetary
discs is only possible at distances larger than $\sim 50$ au from the central
star, requiring high disc masses and accretion rates, and that therefore disc
fragmentation results in the production of brown dwarfs rather than gas giant
planets. However, the effects of dust growth on opacity can be very significant
but have not been taken into account systematically in the models of
fragmenting discs. We employ dust opacity that depends on both temperature and
maximum grain size to evaluate analytically the properties of a critically
fragmenting protoplanetary disc. We find that dust growth may promote disc
fragmentation at disc radii as small as $\sim 30$ au. As a result, the critical
disc masses and accretion rates are smaller, and the initial fragment masses
are in the gas giant planet mass regime. While this suggests that formation of
gas giant planets by disc fragmentation may be more likely than usually
believed, we caution that numerical models of the process are needed to
evaluate the effects not taken into account here, e.g., dust grain mobility and
fragment evolution after disc fragmentation.",http://arxiv.org/pdf/2509.09305v1
2509.09303v1,From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models,"Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.",http://arxiv.org/pdf/2509.09303v1
2509.09299v1,"Towards Efficient and Secure Cloud Control Systems: Advances, Challenges, and Future Directions","Networked Control Systems (NCSs) have been instrumental in realizing fully
connected and responsive intelligent environments within the context of
real-time virtual control and management. However, traditional NCSs face
considerable challenges in handling the vast amounts of data generated by
large-scale control applications, particularly in terms of data acquisition,
storage, and computational processing. To address these challenges, the
emergence of cloud computing and advancements in control theory have empowered
the new paradigm known as Cloud Control Systems (CCSs). Recently, CCSs have
received substantial attention from industries for their potential properties,
such as large-scale data management, complex computations, and data-centric
optimized decisions. This study presents an extensive review of recent progress
in CCSs spanning over multiple studies published between 2012 and 2025.
Specifically, the focus is on providing a taxonomy of the current findings in
CCS research, encompassing various perspectives, such as its efficient
implementations in industrial automation, security and privacy considerations,
and cloud-based control techniques. Each category is examined in depth through
selected state-of-the-art analyses of different approaches and contrasting
methodologies. Furthermore, we discuss future directions aimed at designing
more efficient and practical CCSs. The insights gained from this study can help
researchers, practitioners, and decision-makers in their domain for effective
CCS design and deployment.",http://arxiv.org/pdf/2509.09299v1
2509.09296v1,Over-the-Air Adversarial Attack Detection: from Datasets to Defenses,"Automatic Speaker Verification (ASV) systems can be used for voice-enabled
applications for identity verification. However, recent studies have exposed
these systems' vulnerabilities to both over-the-line (OTL) and over-the-air
(OTA) adversarial attacks. Although various detection methods have been
proposed to counter these threats, they have not been thoroughly tested due to
the lack of a comprehensive data set. To address this gap, we developed the
AdvSV 2.0 dataset, which contains 628k samples with a total duration of 800
hours. This dataset incorporates classical adversarial attack algorithms, ASV
systems, and encompasses both OTL and OTA scenarios. Furthermore, we introduce
a novel adversarial attack method based on a Neural Replay Simulator (NRS),
which enhances the potency of adversarial OTA attacks, thereby presenting a
greater threat to ASV systems. To defend against these attacks, we propose
CODA-OCC, a contrastive learning approach within the one-class classification
framework. Experimental results show that CODA-OCC achieves an EER of 11.2% and
an AUC of 0.95 on the AdvSV 2.0 dataset, outperforming several state-of-the-art
detection methods.",http://arxiv.org/pdf/2509.09296v1
2509.09292v1,LightAgent: Production-level Open-source Agentic AI Framework,"With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",http://arxiv.org/pdf/2509.09292v1
2509.09291v1,What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection,"The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.",http://arxiv.org/pdf/2509.09291v1
2509.09290v1,Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training,"Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG",http://arxiv.org/pdf/2509.09290v1
2509.09289v1,Toward Quantum Enabled Solutions for Real-Time Currency Arbitrage in Financial Markets,"Currency arbitrage leverages price discrepancies in currency exchange rates
across different currency pairs to gain risk-free profits. It involves multiple
trading, where short-lived price discrepancies require real-time, high-speed
processing of vast solution space, posing challenges for classical computing.
In this work, we formulate an enhanced mathematical model for the currency
arbitrage problem by adding simple cycle preservation constraints, which
guarantee trading cycle validity and eliminate redundant or infeasible
substructures. To solve this model, we use and benchmark various solvers,
including Quantum Annealing (QA), gate-based quantum approaches such as
Variational Quantum Algorithm with Adaptive Cost Encoding (ACE), as well as
classical solvers such as Gurobi and classical meta heuristics such as Tabu
Search (TS). We propose a classical multi-bit swap post-processing to improve
the solution generated by ACE. Using real-world currency exchange data, we
compare these methods in terms of both arbitrage profit and execution time, the
two key performance metrics. Our results give insight into the current
capabilities and limitations of quantum methods for real-time financial use
cases.",http://arxiv.org/pdf/2509.09289v1
2509.09286v1,Visual Programmability: A Guide for Code-as-Thought in Chart Understanding,"Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.",http://arxiv.org/pdf/2509.09286v1
2509.09284v1,Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning,"Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.",http://arxiv.org/pdf/2509.09284v1
2509.09282v1,On the Relation of Characteristic Modes of Different Conducting Structures,"A formalism is derived to analyze the scattering of a conducting structure
based on the characteristic modes of another structure whose surface is a
superset of the first structure. This enables the analysis and comparison of
different structures using a common basis of characteristic modes.
Additionally, it is shown that the scattering matrices and perturbation
matrices are no longer diagonal in these cases. Based on this, a modal
transformation matrix is defined to describe the mapping between the
characteristic fields and the weighting coefficients of the two structures.
This matrix enables the conversion of the perturbation matrices in different
bases. Finally, two examples are provided along with a discussion of some
aspects of the theory. The first example aims to validate and illustrate the
formalism. The second example shows how the formalism can be applied in the
design process of an antenna element that is gradually modified, starting from
a base structure.",http://arxiv.org/pdf/2509.09282v1
2509.09278v1,Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations,"Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.",http://arxiv.org/pdf/2509.09278v1
2509.09275v1,Neural Transformer Backflow for Solving Momentum-Resolved Ground States of Strongly Correlated Materials,"Strongly correlated materials, such as twisted transition-metal
dichalcogenide homobilayers, host a variety of exotic quantum phases but remain
notoriously difficult to solve due to strong interactions. We introduce a
powerful neural network ansatz, Neural Transformer Backflow (NTB), formulated
within a multi-band projection framework. It naturally enforces momentum
conservation and enables efficient calculations of momentum-resolved ground
states. NTB attains high accuracy on small systems and scales to higher bands
and larger system sizes far beyond the reach of exact diagonalization. By
evaluating observables such as the structure factor and momentum distribution,
we show that NTB captures diverse correlated states in tMoTe$_2$, including
charge density waves, fractional Chern insulators, and anomalous Hall Fermi
liquids, within a unified framework. Our approach paves the way for
understanding and discovering novel phases of matter in strongly correlated
materials.",http://arxiv.org/pdf/2509.09275v1
2509.09272v1,Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs,"Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.",http://arxiv.org/pdf/2509.09272v1
2509.09269v1,The role of communication delays in the optimal control of spatially invariant systems,"We study optimal proportional feedback controllers for spatially invariant
systems when the controller has access to delayed state measurements received
from different spatial locations. We analyze how delays affect the spatial
locality of the optimal feedback gain leveraging the problem decoupling in the
spatial frequency domain. For the cases of expensive control and small delay,
we provide exact expressions of the optimal controllers in the limit for
infinite control weight and vanishing delay, respectively. In the expensive
control regime, the optimal feedback control law decomposes into a delay-aware
filtering of the delayed state and the optimal controller in the delay-free
setting. Under small delays, the optimal controller is a perturbation of the
delay-free one which depends linearly on the delay. We illustrate our
analytical findings with a reaction-diffusion process over the real line and a
multi-agent system coupled through circulant matrices, showing that delays
reduce the effectiveness of optimal feedback control and may require each
subsystem within a distributed implementation to communicate with farther-away
locations.",http://arxiv.org/pdf/2509.09269v1
2509.09268v1,Ultrafast probing of isotope-induced explciit symmetry breaking in ethylene,"Symmetry governs nature's law, yet many of the natural phenomena occur due to
the breakdown of symmetry. Here, we show how isotope-induced inversion symmetry
breaking influences ultrafast photoisomerization processes in ethylene. Using
extreme ultraviolet pump-near infrared probe time-of-flight mass spectrometry,
we find that replacing one of the carbon atoms in ethylene with a 13C isotope
leads to twice-faster structural relaxation via ethylene-ethylidene
isomerization in the photo-excited molecular cation. Advanced trajectory
surface hopping calculations incorporating the nuclear symmetry of the
molecular systems, reveal that it arises from the mixing of different normal
modes in the isotope-substituted species, interactions otherwise forbidden by
symmetry. Although the mixing does not alter the symmetry of the electronic
Hamiltonian, it modifies that of the nuclear Hamiltonian, causing explicit
symmetry breaking. This facilitates efficient intra-molecular vibrational
energy redistribution, lowering the isomerization yield. Our findings offer
opportunities to use isotope-induced nuclear symmetry breaking to control the
outcome of light-molecule interactions across ultrafast timescales.",http://arxiv.org/pdf/2509.09268v1
2509.09265v1,Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents,"In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/",http://arxiv.org/pdf/2509.09265v1
2509.09264v1,Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG,"Electroencephalography (EEG) signal cleaning has long been a critical
challenge in the research community. The presence of artifacts can
significantly degrade EEG data quality, complicating analysis and potentially
leading to erroneous interpretations. While various artifact rejection methods
have been proposed, the gold standard remains manual visual inspection by human
experts-a process that is time-consuming, subjective, and impractical for
large-scale EEG studies. Existing techniques are often hindered by a strong
reliance on manual hyperparameter tuning, sensitivity to outliers, and high
computational costs. In this paper, we introduce the improved Riemannian Potato
Field (iRPF), a fast and fully automated method for EEG artifact rejection that
addresses key limitations of current approaches. We evaluate iRPF against
several state-of-the-art artifact rejection methods, using two publicly
available EEG databases, labeled for various artifact types, comprising 226 EEG
recordings. Our results demonstrate that iRPF outperforms all competitors
across multiple metrics, with gains of up to 22% in recall, 102% in
specificity, 54% in precision, and 24% in F1-score, compared to Isolation
Forest, Autoreject, Riemannian Potato, and Riemannian Potato Field,
respectively. Statistical analysis confirmed the significance of these
improvements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most
comparisons. Additionally, on a typical EEG recording iRPF performs artifact
cleaning in under 8 milliseconds per epoch using a standard laptop,
highlighting its efficiency for large-scale EEG data processing and real-time
applications. iRPF offers a robust and data-driven artifact rejection solution
for high-quality EEG pre-processing in brain-computer interfaces and clinical
neuroimaging applications.",http://arxiv.org/pdf/2509.09264v1
2509.09263v1,DATE: Dynamic Absolute Time Enhancement for Long Video Understanding,"Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.",http://arxiv.org/pdf/2509.09263v1
2509.09258v1,Intermittent chaos in an optomechanical resonator,"Chaos is a fundamental phenomenon in nonlinear dynamics, manifesting as
irregular and unpredictable behavior across various physical systems. Among the
diverse routes to chaos, intermittent chaos is a distinct transition pathway,
characterized by the temporal or spatial alternation between periodic and
chaotic motions. Here, we experimentally demonstrate, for the first time,
optomechanically induced intermittent chaos in an optical
whispering-gallery-mode microresonator. Specifically, the system evolves from
stable periodic oscillation through an intermittent-chaos regime before fully
developing into chaotic motion. As system parameters vary, the proportion of
chaotic motion in the time-domain increases asymptotically until chaotic
dynamics dominates entirely. Moreover, it is counterintuitive that,
intermittent chaos can act as noise of a favorable intensity compared with
purely periodic or fully chaotic states, and enhance rather than reduce
system's responses in nonlinear ultrasonic detection. These findings not only
deepen the comprehensive understanding of chaos formation but also broaden its
potential applications in high-precision sensing and information processing.",http://arxiv.org/pdf/2509.09258v1
2509.09254v1,Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis,"Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.",http://arxiv.org/pdf/2509.09254v1
2509.09245v1,Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search,"Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.",http://arxiv.org/pdf/2509.09245v1
2509.09241v1,A novel method and dataset for depth-guided image deblurring from smartphone Lidar,"Modern smartphones are equipped with Lidar sensors providing depth-sensing
capabilities. Recent works have shown that this complementary sensor allows to
improve various tasks in image processing, including deblurring. However, there
is a current lack of datasets with realistic blurred images and paired mobile
Lidar depth maps to further study the topic. At the same time, there is also a
lack of blind zero-shot methods that can deblur a real image using the depth
guidance without requiring extensive training sets of paired data. In this
paper, we propose an image deblurring method based on denoising diffusion
models that can leverage the Lidar depth guidance and does not require training
data with paired Lidar depth maps. We also present the first dataset with real
blurred images with corresponding Lidar depth maps and sharp ground truth
images, acquired with an Apple iPhone 15 Pro, for the purpose of studying
Lidar-guided deblurring. Experimental results on this novel dataset show that
Lidar guidance is effective and the proposed method outperforms
state-of-the-art deblurring methods in terms of perceptual quality.",http://arxiv.org/pdf/2509.09241v1
2509.09235v1,Virtual staining for 3D X-ray histology of bone implants,"Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.",http://arxiv.org/pdf/2509.09235v1
2509.09234v1,Agentic LLMs for Question Answering over Tabular Data,"Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.",http://arxiv.org/pdf/2509.09234v1
2509.09232v1,"Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement","In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.",http://arxiv.org/pdf/2509.09232v1
2509.09229v1,Reading Between the Lines: Classifying Resume Seniority with Large Language Models,"Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt",http://arxiv.org/pdf/2509.09229v1
2509.09227v1,Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery,"Purpose: To introduce novel dynamic structural parameters and evaluate their
integration within a multimodal deep learning (DL) framework for predicting
postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)
patients. Methods: We utilized a publicly available longitudinal OCT dataset at
five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage
specific segmentation model delineated related structures, and an automated
pipeline extracted quantitative, composite, qualitative, and dynamic features.
Binary logistic regression models, constructed with and without dynamic
parameters, assessed their incremental predictive value for best-corrected
visual acuity (BCVA). A multimodal DL model combining clinical variables,
OCT-derived features, and raw OCT images was developed and benchmarked against
regression models. Results: The segmentation model achieved high accuracy
across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses
identified base diameter, ellipsoid zone integrity, and macular hole area as
significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates
consistently improved logistic regression AUC, especially at the 3-month
follow-up. The multimodal DL model outperformed logistic regression, yielding
higher AUCs and overall accuracy at each stage. The difference is as high as
0.12, demonstrating the complementary value of raw image volume and dynamic
parameters. Conclusions: Integrating dynamic parameters into the multimodal DL
model significantly enhances the accuracy of predictions. This fully automated
process therefore represents a promising clinical decision support tool for
personalized postoperative management in macular hole surgery.",http://arxiv.org/pdf/2509.09227v1
2509.09225v1,On Sampling of Multiple Correlated Stochastic Signals,"Multiple stochastic signals possess inherent statistical correlations, yet
conventional sampling methods that process each channel independently result in
data redundancy. To leverage this correlation for efficient sampling, we model
correlated channels as a linear combination of a smaller set of uncorrelated,
wide-sense stationary latent sources. We establish a theoretical lower bound on
the total sampling density for zero mean-square error reconstruction, proving
it equals the ratio of the joint spectral bandwidth of latent sources to the
number of correlated signal channels. We then develop a constructive multi-band
sampling scheme that attains this bound. The proposed method operates via
spectral partitioning of the latent sources, followed by spatio-temporal
sampling and interpolation. Experiments on synthetic and real datasets confirm
that our scheme achieves near-lossless reconstruction precisely at the
theoretical sampling density, validating its efficiency.",http://arxiv.org/pdf/2509.09225v1
2509.09224v1,Magnetic White Dwarf -- M Dwarf Binaries in Pre-polar Phase as Special Population of Long-Period Radio Transients,"Long-period radio transients (LPTs) are a new class of coherent radio sources
with periods ranging from minutes to hours. Recently, two LPT sources, ILT
J1101+5521 and GLEAM-X J0704-37, with periods of 2-3 hours has been confirmed
to originate from white dwarf (WD) -- M dwarf (MD) binaries. In this work, we
propose that at least some LPTs originate from the magnetic WD -- MD binaries
in the pre-polar phase. The asynchronism between the WD's rotation and the
binary's orbital motion allows for the unipolar-inductor mechanism or
magnetosphere interaction to operate and accelerate radiating particles, with
the dominant process depending on the magnetic moment ratio of the two stars.
Under asynchronism condition, both the peak flux and the polarization of radio
pulses will be modulated by the beat period. The pre-polar phase characterized
by an extremely low accretion rate provides the relatively clean magnetospheric
environment necessary for a loss-cone-driven maser (LCDM) mechanism to operate,
producing the LPT emission. The observed pulse duty cycle of $10^{-3}-10^{-1}$
is attributed to a beaming effect modulated by the binary's magnetic geometry.
Furthermore, the magnetized environment of a WD--MD binary is conducive to
Faraday conversion with weak coupling, which implies that the polarization
state of LPTs should vary significantly at different periods. Finally, we
predict that LPTs from WD--MD binaries should exhibit a period distribution
following $f_P(P)dP \propto P^{-(1.67-2.33)}dP$ and a luminosity function
described by $f_L(L)dL \propto L^{-(1.80-2.67)}dL$, which can be tested by the
future large sample.",http://arxiv.org/pdf/2509.09224v1
2509.09221v1,A hybrid quantum walk model unifying discrete and continuous quantum walks,"Quantum walks, both discrete and continuous, serve as fundamental tools in
quantum information processing with diverse applications. This work introduces
a hybrid quantum walk model that integrates the coin mechanism of discrete
walks with the Hamiltonian-driven time evolution of continuous walks. Through
systematic analysis of probability distributions, standard deviations, and
entanglement entropy on fundamental graph structures (2-vertex circles, stars,
and lines), we reveal distinctive dynamical characteristics that differentiate
our model from conventional quantum walk paradigms. The proposed framework
demonstrates unifying capabilities by naturally encompassing existing quantum
walk models as special cases. Two significant applications emerge from this
hybrid architecture: (1) We develop a novel protocol for perfect state
transfer(PST) in general connected graphs, overcoming the limitations of
previous graph-specific approaches. A PST on a tree graph has been implemented
on a quantum superconducting processor. (2) We devise a quantum algorithm for
multiplying $K$ adjacency matrices of $n$-vertex regular graphs with time
complexity $O(n^2d_1\cdots d_K)$, outperforming classical matrix multiplication
$(O(n^{2.371552}))$ when vertex degrees $d_i$ are bounded. The algorithm's
efficacy for triangle counting is experimentally validated through the quantum
simulation on PennyLane. These results establish the hybrid quantum walk as a
versatile framework bridging discrete and continuous paradigms while enabling
practical quantum advantage in graph computation tasks.",http://arxiv.org/pdf/2509.09221v1
2509.09218v1,Guarded Fragments Meet Dynamic Logic: The Story of Regular Guards (Extended Version),"We study the Guarded Fragment with Regular Guards (RGF), which combines the
expressive power of the Guarded Fragment (GF) with Propositional Dynamic Logic
with Intersection and Converse (ICPDL). Our logic generalizes, in a uniform
way, many previously-studied extensions of GF, including (conjunctions of)
transitive or equivalence guards, transitive or equivalence closure and more.
We prove 2EXPTIME-completeness of the satisfiability problem for RGF, showing
that RGF is not harder than ICPDL or GF. Shifting to the query entailment
problem, we provide undecidability results that significantly strengthen and
solidify earlier results along those lines. We conclude by identifying, in a
natural sense, the maximal EXPSPACE-complete fragment of RGF.",http://arxiv.org/pdf/2509.09218v1
2509.09215v1,"Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions","Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.",http://arxiv.org/pdf/2509.09215v1
2509.09214v1,Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach,"Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.",http://arxiv.org/pdf/2509.09214v1
2509.09213v1,A novel cost-effective fabrication of a flexible neural probe for brain signal recording,"This study introduces a novel, flexible, and implantable neural probe using a
cost-effective microfabrication process based on a thin polyimide film.
Polyimide film, known as Kapton, serves as a flexible substrate for
microelectrodes, conductive tracks, and contact pads of the probe, which are
made from a thin film of gold (Au). SU-8 is used to cover the corresponding
tracks for electrical isolation and to increase the stiffness of the probe for
better implantation. To evaluate the performance of the fabricated probe,
electrochemical impedance spectroscopy (EIS) and artificial neural signal
recording have been used to characterize its properties. The microelectrode
dimensions have been carefully chosen to provide low impedance characteristics,
which are necessary for acquiring local field potential (LFP) signals. The in
vivo LFP data have been obtained from a male zebra finch presented with
auditory stimuli. By properly filtering the extracellular recordings and
analyzing the data, the obtained results have been validated by comparing them
with the signals acquired with a commercial neural electrode. Due to the use of
Kapton, SU-8, and Au materials with non-toxic and adaptable properties in the
body environment, the fabricated neural probe is considered a promising
biocompatible implantable neural probe that may pave the way for the
fabrication of other neural implantable devices with commercial aims.",http://arxiv.org/pdf/2509.09213v1
2509.09212v1,MAPSS: Manifold-based Assessment of Perceptual Source Separation,"Objective assessment of source-separation systems still mismatches subjective
human perception, especially when leakage and self-distortion interact. We
introduce the Perceptual Separation (PS) and Perceptual Match (PM), the first
pair of measures that functionally isolate these two factors. Our intrusive
method begins with generating a bank of fundamental distortions for each
reference waveform signal in the mixture. Distortions, references, and their
respective system outputs from all sources are then independently encoded by a
pre-trained self-supervised learning model. These representations are
aggregated and projected onto a manifold via diffusion maps, which aligns
Euclidean distances on the manifold with dissimilarities of the encoded
waveforms. On this manifold, the PM measures the Mahalanobis distance from each
output to its attributed cluster that consists of its reference and distortions
embeddings, capturing self-distortion. The PS accounts for the Mahalanobis
distance of the output to the attributed and to the closest non-attributed
clusters, quantifying leakage. Both measures are differentiable and granular,
operating at a resolution as low as 50 frames per second. We further derive,
for both measures, deterministic error radius and non-asymptotic,
high-probability confidence intervals (CIs). Experiments on English, Spanish,
and music mixtures show that the PS and PM nearly always achieve the highest
linear correlation coefficients with human mean-opinion scores than 14
competitors, reaching as high as 86.36% for speech and 87.21% for music. We
observe, at worst, an error radius of 1.39% and a probabilistic 95% CI of
12.21% for these coefficients, which improves reliable and informed evaluation.
Using mutual information, the measures complement each other most as their
values decrease, suggesting they are jointly more informative as system
performance degrades.",http://arxiv.org/pdf/2509.09212v1
2509.09210v1,ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting,"Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.",http://arxiv.org/pdf/2509.09210v1
2509.09209v1,The open XXZ chain at $=-1/2$ and totally-symmetric alternating sign matrices,"The open XXZ spin chain with the anisotropy parameter $\Delta=-\frac12$,
diagonal boundary fields that depend on a parameter $x$, and finite length $N$
is studied. In a natural normalisation, the components of its ground-state
vector are polynomials in $x$ with integer coefficients. It is shown that their
sum is given by a generating function for the weighted enumeration of
totally-symmetric alternating sign matrices with weights depending on $x$.",http://arxiv.org/pdf/2509.09209v1
2509.09204v1,Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems,"Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.",http://arxiv.org/pdf/2509.09204v1
2509.09199v1,CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling,"Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\""ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.",http://arxiv.org/pdf/2509.09199v1
2509.09198v1,GmSLM : Generative Marmoset Spoken Language Modeling,"Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.",http://arxiv.org/pdf/2509.09198v1
2509.09197v1,Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function,"Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.",http://arxiv.org/pdf/2509.09197v1
2509.09196v1,Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition,"Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives ""bonus scores"" to partial hypothesis (e.g.
""Bon"") that may lead to the generation of the rare word (e.g. ""Bonham""). If the
full word (""Bonham"") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.",http://arxiv.org/pdf/2509.09196v1
2509.09194v1,On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability,"Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.",http://arxiv.org/pdf/2509.09194v1
2509.09193v1,AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives,"Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.",http://arxiv.org/pdf/2509.09193v1
2509.09192v1,"Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset","Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.",http://arxiv.org/pdf/2509.09192v1
2509.09191v1,Permutation-Based Distances for Groups and Group-Valued Time Series,"Permutations on a set, endowed with function composition, build a group
called a symmetric group. In addition to their algebraic structure, symmetric
groups have two metrics that are of particular interest to us here: the Cayley
distance and the Kendall tau distance. In fact, the aim of this paper is to
introduce the concept of distance in a general finite group based on them. The
main tool that we use to this end is Cayley's theorem, which states that any
finite group is isomorphic to a subgroup of a certain symmetric group. We also
discuss the advantages and disadvantage of these permutation-based distances
compared to the conventional generator-based distances in finite groups. The
reason why we are interested in distances on groups is that finite groups
appear in symbolic representations of time series, most notably in the
so-called ordinal representations, whose symbols are precisely permutations,
usually called ordinal patterns in that context. The natural extension from
groups to group-valued time series is also discussed, as well as how such
metric tools can be applied in time series analysis. Both theory and
applications are illustrated with examples and numerical simulations.",http://arxiv.org/pdf/2509.09191v1
2509.09188v1,Moments of additive martingales of branching Lvy processes and applications,"Let $W_t(\theta)$ be the Biggins martingale of a supercritical branching
L\'evy process with non-local branching mechanism, and denote by
$W_\infty(\theta)$ its limit. In this paper, we first study moment properties
of $W_t(\theta)$ and $W_\infty(\theta)$, and the tail behavior of
$W_\infty(\theta)$. We then apply these results to establish central limit
theorems for $W_t(\theta)-W_\infty(\theta)$.",http://arxiv.org/pdf/2509.09188v1
2509.09185v1,Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit,"In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.",http://arxiv.org/pdf/2509.09185v1
2509.09183v1,Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection,"Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.",http://arxiv.org/pdf/2509.09183v1
2509.09179v1,Diffraction-Unlimited Tip-Enhanced Sum-Frequency Vibrational Nanoscopy,"Sum-frequency generation (SFG) is a powerful second-order nonlinear
spectroscopic technique that provides detailed insights into molecular
structures and absolute orientations at surfaces and interfaces. However,
conventional SFG based on far-field schemes suffers from the diffraction limit
of light, which inherently averages spectroscopic information over
micrometer-scale regions and obscures nanoscale structural inhomogeneity. Here,
we overcome this fundamental limitation by leveraging a highly confined optical
near field within a tip-substrate nanogap of a scanning tunneling microscope
(STM), pushing the spatial resolution of SFG down to ~10 nm, a nearly
two-orders-of-magnitude improvement over conventional far-field SFG. By
capturing tip-enhanced SFG (TE-SFG) spectra concurrently with STM scanning, we
demonstrate the capability to resolve nanoscale variation in molecular
adsorption structures across distinct interfacial domains. To rigorously
interpret the observed TE-SFG spectra, we newly developed a comprehensive
theoretical framework for the TE-SFG process and confirm via numerical
simulations that the TE-SFG response under our current experimental conditions
is dominantly governed by the dipole-field interactions, with negligible
contributions from higher-order multipole effects. The dominance of the dipole
mechanism ensures that the observed TE-SFG spectra faithfully reflect not only
nanoscale interfacial structural features but also absolute up/down molecular
orientations. This study presents the first experimental realization of
diffraction-unlimited second-order nonlinear vibrational SFG nanoscopy, opening
a new avenue for nanoscale domain-specific investigation of molecular
structures and dynamics within inhomogeneous interfacial molecular systems
beyond the conventional far-field SFG and STM imaging.",http://arxiv.org/pdf/2509.09179v1
2509.09174v1,EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs,"Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.",http://arxiv.org/pdf/2509.09174v1
2509.09172v1,Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios,"With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.",http://arxiv.org/pdf/2509.09172v1
2509.09171v1,Structural Complexity and Correlated Disorder in Materials Chemistry,"Complexity is a measure of information content. Crystalline materials are not
complex systems because their structures can be represented tersely using the
language of crystallography. Disordered materials are also structurally simple
if the disorder present is random: such systems can be described efficiently
through statistical mechanics. True complexity emerges when structures are
neither perfectly crystalline nor randomly disordered -- a middle ground once
named ``organised complexity''. In current parlance, in our field, we use the
term ``correlated disorder'' for this same regime, emphasising the presence and
importance of non-random patterns.",http://arxiv.org/pdf/2509.09171v1
2509.09168v1,Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication,"Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.",http://arxiv.org/pdf/2509.09168v1
2509.09162v1,"Divide, Interact, Sample: The Two-System Paradigm","Mean-field, ensemble-chain, and adaptive samplers have historically been
viewed as distinct approaches to Monte Carlo sampling. In this paper, we
present a unifying {two-system} framework that brings all three under one roof.
In our approach, an ensemble of particles is split into two interacting
subsystems that propose updates for each other in a symmetric, alternating
fashion. This cross-system interaction ensures that the overall ensemble has
$\rho(x)$ as its invariant distribution in both the finite-particle setting and
the mean-field limit. The two-system construction reveals that ensemble-chain
samplers can be interpreted as finite-$N$ approximations of an ideal mean-field
sampler; conversely, it provides a principled recipe to discretize mean-field
Langevin dynamics into tractable parallel MCMC algorithms. The framework also
connects naturally to adaptive single-chain methods: by replacing
particle-based statistics with time-averaged statistics from a single chain,
one recovers analogous adaptive dynamics in the long-time limit without
requiring a large ensemble. We derive novel two-system versions of both
overdamped and underdamped Langevin MCMC samplers within this paradigm. Across
synthetic benchmarks and real-world posterior inference tasks, these two-system
samplers exhibit significant performance gains over the popular No-U-Turn
Sampler, achieving an order of magnitude higher effective sample sizes per
gradient evaluation.",http://arxiv.org/pdf/2509.09162v1
2509.09160v1,Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing,"Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.",http://arxiv.org/pdf/2509.09160v1
2509.09159v1,A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering,"Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.",http://arxiv.org/pdf/2509.09159v1
2509.09157v1,RT-DETR++ for UAV Object Detection,"Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.",http://arxiv.org/pdf/2509.09157v1
2509.09155v1,HISPASpoof: A New Dataset For Spanish Speech Forensics,"Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.",http://arxiv.org/pdf/2509.09155v1
2509.09154v1,Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective,"Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.",http://arxiv.org/pdf/2509.09154v1
2509.09152v1,LITcoder: A General-Purpose Library for Building and Comparing Encoding Models,"We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io",http://arxiv.org/pdf/2509.09152v1
2509.09149v1,Automotive sound field reproduction using deep optimization with spatial domain constraint,"Sound field reproduction with undistorted sound quality and precise spatial
localization is desirable for automotive audio systems. However, the complexity
of automotive cabin acoustic environment often necessitates a trade-off between
sound quality and spatial accuracy. To overcome this limitation, we propose
Spatial Power Map Net (SPMnet), a learning-based sound field reproduction
method that improves both sound quality and spatial localization in complex
environments. We introduce a spatial power map (SPM) constraint, which
characterizes the angular energy distribution of the reproduced field using
beamforming. This constraint guides energy toward the intended direction to
enhance spatial localization, and is integrated into a multi-channel
equalization framework to also improve sound quality under reverberant
conditions. To address the resulting non-convexity, deep optimization that use
neural networks to solve optimization problems is employed for filter design.
Both in situ objective and subjective evaluations confirm that our method
enhances sound quality and improves spatial localization within the automotive
cabin. Furthermore, we analyze the influence of different audio materials and
the arrival angles of the virtual sound source in the reproduced sound field,
investigating the potential underlying factors affecting these results.",http://arxiv.org/pdf/2509.09149v1
2509.09147v1,JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information,"Wiener filtering in the joint time-vertex fractional Fourier transform
(JFRFT) domain has shown high effectiveness in denoising time-varying graph
signals. Traditional filtering models use grid search to determine the
transform-order pair and compute filter coefficients, while learnable ones
employ gradient-descent strategies to optimize them; both require complete
prior information of graph signals. To overcome this shortcoming, this letter
proposes a data-model co-driven denoising approach, termed neural-network-aided
joint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the
JFRFT-domain Wiener filter model into a neural network and updates the
transform-order pair and filter coefficients through a data-driven approach.
This design enables effective denoising using only partial prior information.
Experiments demonstrate that JFRFFNet achieves significant improvements in
output signal-to-noise ratio compared with some state-of-the-art methods.",http://arxiv.org/pdf/2509.09147v1
2509.09146v1,Peering Partner Recommendation for ISPs using Machine Learning,"Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.",http://arxiv.org/pdf/2509.09146v1
2509.09144v1,Sequential Spectral Clustering of Data Sequences,"We study the problem of nonparametric clustering of data sequences, where
each data sequence comprises i.i.d. samples generated from an unknown
distribution. The true clusters are the clusters obtained using the Spectral
clustering algorithm (SPEC) on the pairwise distance between the true
distributions corresponding to the data sequences. Since the true distributions
are unknown, the objective is to estimate the clusters by observing the minimum
number of samples from the data sequences for a given error probability. To
solve this problem, we propose the Sequential Spectral clustering algorithm
(SEQ-SPEC), and show that it stops in finite time almost surely and is
exponentially consistent. We also propose a computationally more efficient
algorithm called the Incremental Approximate Sequential Spectral clustering
algorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed
algorithms perform better than the fixed sample size SPEC, the Sequential
$K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage
clustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally
efficient, performs close to SEQ-SPEC on both synthetic and real-world
datasets. To the best of our knowledge, this is the first work on spectral
clustering of data sequences under a sequential framework.",http://arxiv.org/pdf/2509.09144v1
2509.09142v1,Shortest-path percolation on scale-free networks,"The shortest-path percolation (SPP) model aims at describing the consumption,
and eventual exhaustion, of a network's resources. Starting from a graph
containing a macroscopic connected component, random pairs of nodes are
sequentially selected, and, if the length of the shortest path connecting the
node pairs is smaller than a tunable budget parameter, then all edges along
such a path are removed from the network. As edges are progressively removed,
the network eventually breaks into multiple microscopic components, undergoing
a percolation-like transition. It is known that SPP transition on
Erd\H{o}s-R\'enyi (ER) graphs belongs to same universality class as of the
ordinary bond percolation if the budget parameter is finite; for unbounded
budget instead, the SPP transition becomes more abrupt than the ordinary
percolation transition. By means of large-scale numerical simulations and
finite-size scaling analysis, here we study the SPP transition on random
scale-free networks (SFNs) characterized by power-law degree distributions. We
find, in contrast with standard percolation, that the transition is identical
to the one observed on ER graphs, denoting independence from the degree
exponent. Still, we distinguish finite- and infinite-budget SPP universality
classes. Our findings follow from the fact that the SPP process drastically
homogenizes the heterogeneous structure of SFNs before the SPP transition takes
place.",http://arxiv.org/pdf/2509.09142v1
